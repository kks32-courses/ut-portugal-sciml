{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 (Part 2): Automatic Differentiation\n",
    "\n",
    "**Exercise:** [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kks32-courses/sciml/blob/main/lectures/07-ad/07-ad-exercise.ipynb)\n",
    "**Solution:** [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kks32-courses/sciml/blob/main/lectures/07-ad/07-ad.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Why Do We Need Better Ways to Compute Derivatives?\n",
    "\n",
    "In scientific computing, we constantly need derivatives. Whether we're solving optimization problems, training neural networks, or inverting geophysical data, derivatives are the mathematical engine that drives our algorithms forward.\n",
    "\n",
    "Traditionally, we've had three approaches to compute derivatives, each with fundamental limitations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Modes of differentiation](figs/differentiation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Four Modes of Differentiation\n",
    "\n",
    "### 1. Manual Differentiation\n",
    "The traditional calculus approach: derive formulas by hand using calculus rules.\n",
    "\n",
    "**Pros**: \n",
    "- Exact derivatives\n",
    "- Allows optimization during derivation\n",
    "\n",
    "**Cons**: \n",
    "- Error-prone and time-consuming\n",
    "- Not scalable for complex functions\n",
    "- Impractical for functions with thousands of parameters\n",
    "\n",
    "### 2. Symbolic Differentiation\n",
    "Let computers apply calculus rules symbolically, like Mathematica or SymPy.\n",
    "\n",
    "**Pros**:\n",
    "- Provides exact, closed-form expressions\n",
    "\n",
    "**Cons**:\n",
    "- **Expression swell**: derivatives can become exponentially large\n",
    "- Not feasible when symbolic form isn't available\n",
    "- Computationally expensive for complex expressions\n",
    "\n",
    "### 3. Numerical Differentiation\n",
    "Approximate derivatives using finite differences:\n",
    "$$ f'(x) \\approx \\frac{f(x + h) - f(x - h)}{2h} $$\n",
    "\n",
    "**Pros**:\n",
    "- Simple to implement\n",
    "- Works for any function you can evaluate\n",
    "\n",
    "**Cons**:\n",
    "- **Accuracy issues**: prone to rounding errors\n",
    "- **Step size dilemma**: too large → truncation error, too small → round-off error\n",
    "- **Computational cost**: requires multiple function evaluations\n",
    "\n",
    "### 4. Automatic Differentiation (AD)\n",
    "The revolutionary approach: compute exact derivatives alongside function evaluation.\n",
    "\n",
    "**Pros**:\n",
    "- **Exact derivatives** (up to machine precision)\n",
    "- **Efficient**: computational cost proportional to function evaluation\n",
    "- **General**: works for any differentiable function expressible as code\n",
    "\n",
    "**Cons**:\n",
    "- Requires specialized libraries\n",
    "- Can be memory-intensive for some applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Core Insight: Functions Are Computational Graphs\n",
    "\n",
    "Every computer program that evaluates a mathematical function can be viewed as a **computational graph**. Consider this simple function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x1, x2):\n",
    "    y = x1**2 + x2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a computational graph where each operation is a node:\n",
    "\n",
    "![AD graph](figs/ad1.png)\n",
    "\n",
    "We can decompose this into elementary operations and assign intermediate variables:\n",
    "\n",
    "![AD graph variables](figs/ad2.png)\n",
    "\n",
    "This decomposition is the key insight that makes automatic differentiation possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Mode Automatic Differentiation\n",
    "\n",
    "Forward mode AD computes derivatives by propagating derivative information **forward** through the computational graph, following the same path as the function evaluation.\n",
    "\n",
    "![AD forward evaluation](figs/ad3.png)\n",
    "\n",
    "### Forward Mode: Computing $\\frac{\\partial y}{\\partial x_1}$\n",
    "\n",
    "Starting with our function $y = x_1^2 + x_2$, let's trace through the computation:\n",
    "\n",
    "1. **Seed the input**: Set $\\dot{x}_1 = 1$ and $\\dot{x}_2 = 0$ (we're differentiating w.r.t. $x_1$)\n",
    "\n",
    "2. **Forward propagation**:\n",
    "   - $v_1 = x_1^2$, so $\\dot{v}_1 = 2x_1 \\cdot \\dot{x}_1 = 2x_1 \\cdot 1 = 2x_1$\n",
    "   - $y = v_1 + x_2$, so $\\dot{y} = \\dot{v}_1 + \\dot{x}_2 = 2x_1 + 0 = 2x_1$\n",
    "\n",
    "3. **Result**: $\\frac{\\partial y}{\\partial x_1} = 2x_1$\n",
    "\n",
    "### Forward Mode: Computing $\\frac{\\partial y}{\\partial x_2}$\n",
    "\n",
    "To get the derivative w.r.t. $x_2$, we seed differently:\n",
    "\n",
    "1. **Seed the input**: Set $\\dot{x}_1 = 0$ and $\\dot{x}_2 = 1$\n",
    "\n",
    "2. **Forward propagation**:\n",
    "   - $v_1 = x_1^2$, so $\\dot{v}_1 = 2x_1 \\cdot \\dot{x}_1 = 2x_1 \\cdot 0 = 0$\n",
    "   - $y = v_1 + x_2$, so $\\dot{y} = \\dot{v}_1 + \\dot{x}_2 = 0 + 1 = 1$\n",
    "\n",
    "3. **Result**: $\\frac{\\partial y}{\\partial x_2} = 1$\n",
    "\n",
    "**Key insight**: Forward mode requires one pass per input variable to compute all partial derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Mode Automatic Differentiation\n",
    "\n",
    "Reverse mode AD (also called **backpropagation**) computes derivatives by propagating derivative information **backward** through the computational graph.\n",
    "\n",
    "![Reverse mode AD](figs/ad4.png)\n",
    "\n",
    "### The Backward Pass Algorithm\n",
    "\n",
    "1. **Forward pass**: Compute function values and store intermediate results\n",
    "2. **Seed the output**: Set $\\bar{y} = 1$ (derivative of output w.r.t. itself)\n",
    "3. **Backward pass**: Use the chain rule to propagate derivatives backward\n",
    "\n",
    "![Chain rule AD](figs/ad5.png)\n",
    "\n",
    "Let's trace through our example:\n",
    "\n",
    "![Chain rule AD steps](figs/ad6.png)\n",
    "\n",
    "![Final chain rule AD](figs/ad7.png)\n",
    "\n",
    "### Computing All Partial Derivatives in One Pass\n",
    "\n",
    "The beauty of reverse mode is that it computes **all** partial derivatives in a single backward pass:\n",
    "\n",
    "1. **Forward pass**: $y = x_1^2 + x_2$ (store intermediate values)\n",
    "\n",
    "2. **Backward pass with $\\bar{y} = 1$**:\n",
    "   - $\\frac{\\partial y}{\\partial x_1} = \\frac{\\partial y}{\\partial v_1} \\cdot \\frac{\\partial v_1}{\\partial x_1} = 1 \\cdot 2x_1 = 2x_1$\n",
    "   - $\\frac{\\partial y}{\\partial x_2} = \\frac{\\partial y}{\\partial x_2} = 1$\n",
    "\n",
    "**Key insight**: Reverse mode computes gradients w.r.t. all inputs in a single backward pass!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Forward vs Reverse Mode\n",
    "\n",
    "The choice depends on the structure of your problem:\n",
    "\n",
    "- **Forward Mode**: Efficient when **few inputs, many outputs** (e.g., $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ with $n \\ll m$)\n",
    "- **Reverse Mode**: Efficient when **many inputs, few outputs** (e.g., $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ with $n \\gg m$)\n",
    "\n",
    "In machine learning, we typically have millions of parameters (inputs) and a single loss function (output), making reverse mode the natural choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation in Practice: PyTorch\n",
    "\n",
    "Let's see how automatic differentiation works in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define variables that require gradients\n",
    "x1 = torch.tensor(2.0, requires_grad=True)\n",
    "x2 = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Define the function\n",
    "y = x1**2 + x2\n",
    "\n",
    "# Compute gradients using reverse mode AD\n",
    "y.backward()\n",
    "\n",
    "# Access the computed gradients\n",
    "print(f\"dy/dx1: {x1.grad.item()}\")  # Should be 2*x1 = 4.0\n",
    "print(f\"dy/dx2: {x2.grad.item()}\")  # Should be 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A More Complex Example: Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2, 3)\n",
    "        self.layer2 = nn.Linear(3, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Create network and data\n",
    "net = SimpleNet()\n",
    "x = torch.tensor([[1.0, 2.0]], requires_grad=True)\n",
    "target = torch.tensor([[0.5]])\n",
    "\n",
    "# Forward pass\n",
    "output = net(x)\n",
    "loss = ((output - target)**2).mean()\n",
    "\n",
    "# Backward pass - computes gradients for ALL parameters\n",
    "loss.backward()\n",
    "\n",
    "# Access gradients\n",
    "for name, param in net.named_parameters():\n",
    "    print(f\"{name}: gradient shape {param.grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of automatic differentiation becomes clear: PyTorch automatically computes gradients for all parameters in the network, regardless of how complex the architecture becomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAX: Functional Automatic Differentiation\n",
    "\n",
    "JAX takes a different, more functional approach to automatic differentiation. Instead of tracking gradients on data structures (like PyTorch Tensors), JAX provides **function transformations** that take a function and return a new, transformed function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Core Idea: Function Transformations\n",
    "\n",
    "JAX's power comes from a few key transformations:\n",
    "\n",
    "- **`jit` (Just-In-Time Compilation)**: Speeds up your code by compiling it to highly optimized machine code using XLA (Accelerated Linear Algebra). This is especially effective for code with loops.\n",
    "- **`grad` (Gradient)**: Takes a function and returns a new function that computes its gradient. This is JAX's implementation of reverse-mode AD.\n",
    "- **`vmap` (Vectorization)**: Automatically vectorizes a function, allowing it to process batches of data without you needing to write explicit loops. It's like adding a batch dimension to your function for free.\n",
    "- **`pmap` (Parallelization)**: Runs computations in parallel across multiple devices (like GPUs or TPUs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Gradients and Hessians\n",
    "\n",
    "Let's see how to use these transformations to get first and second-order derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradients of Multi-Argument Functions\n",
    "\n",
    "The `grad` function is incredibly flexible. For a function with multiple inputs, you can specify which argument to differentiate with respect to using `argnums`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function with two arguments\n",
    "def my_func(x, y):\n",
    "    return x**3 * y + 2*x*y**2\n",
    "\n",
    "# Create a function that computes the gradient w.r.t. x (the 0-th argument)\n",
    "grad_x = jax.grad(my_func, argnums=0)\n",
    "\n",
    "# Create a function that computes the gradient w.r.t. y (the 1st argument)\n",
    "grad_y = jax.grad(my_func, argnums=1)\n",
    "\n",
    "# Evaluate at a point (x=2, y=3)\n",
    "x_val, y_val = 2.0, 3.0\n",
    "df_dx = grad_x(x_val, y_val)\n",
    "df_dy = grad_y(x_val, y_val)\n",
    "\n",
    "print(f\"Original function f(2, 3) = {my_func(x_val, y_val)}\")\n",
    "print(f\"∂f/∂x at (2, 3) = {df_dx}\") # Analytical: 3*x^2*y + 2*y^2 = 3*4*3 + 2*9 = 36 + 18 = 54\n",
    "print(f\"∂f/∂y at (2, 3) = {df_dy}\") # Analytical: x^3 + 4*x*y = 8 + 4*2*3 = 8 + 24 = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Higher-Order Derivatives: Hessians\n",
    "\n",
    "Because JAX transformations are composable, computing higher-order derivatives is straightforward. The Hessian is the Jacobian of the gradient. We can compute it by composing `jax.jacfwd` (or `jax.jacrev`) with `jax.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a simpler function for clarity\n",
    "def scalar_func(x):\n",
    "    return x[0]**2 * x[1] + x[1]**3\n",
    "\n",
    "# The Hessian is the jacobian of the gradient\n",
    "hessian_func = jax.jacfwd(jax.grad(scalar_func))\n",
    "\n",
    "# Evaluate at a point\n",
    "point = jnp.array([2.0, 3.0])\n",
    "hessian_matrix = hessian_func(point)\n",
    "\n",
    "print(\"Hessian matrix at (2, 3):\")\n",
    "print(hessian_matrix)\n",
    "\n",
    "# --- For verification ---\n",
    "# Gradient: ∇f = [2xy, x^2 + 3y^2]\n",
    "# At (2,3): ∇f = [12, 4 + 27] = [12, 31]\n",
    "#\n",
    "# Hessian: H = [[∂²f/∂x², ∂²f/∂y∂x],\n",
    "#               [∂²f/∂x∂y, ∂²f/∂y²]]\n",
    "#\n",
    "# H = [[2y, 2x],\n",
    "#      [2x, 6y]]\n",
    "#\n",
    "# At (2,3): H = [[6, 4],\n",
    "#               [4, 18]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Considerations\n",
    "\n",
    "### Memory vs Computation Trade-offs\n",
    "\n",
    "**Forward Mode**:\n",
    "- Memory: O(1) additional storage\n",
    "- Computation: O(n) for n input variables\n",
    "\n",
    "**Reverse Mode**:\n",
    "- Memory: O(computation graph size)\n",
    "- Computation: O(1) for any number of input variables\n",
    "\n",
    "### Modern Optimizations\n",
    "\n",
    "1. **Checkpointing**: Trade computation for memory by recomputing intermediate values\n",
    "2. **JIT compilation**: Compile computational graphs for faster execution\n",
    "3. **Parallelization**: Distribute gradient computation across multiple devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mathematical Foundation\n",
    "\n",
    "Automatic differentiation works because of a fundamental theorem:\n",
    "\n",
    "**Chain Rule**: For composite functions $f(g(x))$:\n",
    "$$\\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x)$$\n",
    "\n",
    "By systematically applying the chain rule to each operation in a computational graph, AD can compute exact derivatives for arbitrarily complex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: The AD Revolution\n",
    "\n",
    "Automatic differentiation has revolutionized scientific computing by making gradients:\n",
    "\n",
    "1. **Ubiquitous**: Available for any differentiable computation\n",
    "2. **Exact**: No approximation errors (up to machine precision)\n",
    "3. **Efficient**: Computational cost proportional to function evaluation\n",
    "4. **Automatic**: No manual derivation required\n",
    "\n",
    "This has enabled new approaches to:\n",
    "- Machine learning (deep learning wouldn't exist without AD)\n",
    "- Scientific computing (differentiable simulators)\n",
    "- Optimization (gradient-based methods for complex problems)\n",
    "- Parameter estimation (automatic gradient computation for inverse problems)\n",
    "\n",
    "The combination of automatic differentiation with modern hardware (GPUs, TPUs) has created unprecedented opportunities for scientific discovery through computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking Forward\n",
    "\n",
    "As we move into the era of scientific machine learning, automatic differentiation serves as the mathematical engine that powers:\n",
    "- Physics-informed neural networks\n",
    "- Neural differential equations  \n",
    "- Differentiable simulators\n",
    "- End-to-end optimization of scientific workflows\n",
    "\n",
    "The ability to compute gradients automatically through arbitrary computational processes is not just a technical convenience—it's a fundamental capability that's reshaping how we approach scientific problems in the 21st century."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
