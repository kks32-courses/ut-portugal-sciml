{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks as Function Approximators\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Having established the 1D Poisson equation as our benchmark problem, we now take the crucial step from mathematical theory to computational practice. The central question driving this exploration is: *How can we train a neural network to approximate an unknown function from sparse data?*\n",
    "\n",
    "This notebook introduces the fundamental concepts of neural network function approximation. We transform the abstract notion of \"learning a function\" into concrete algorithms, revealing both the power and initial limitations of simple network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Framework\n",
    "\n",
    "Consider a target function $u^*(x)$ that we wish to approximate. In our case, this is the solution to the 1D Poisson equation:\n",
    "$$u^*(x) = \\sin(\\pi x)$$\n",
    "\n",
    "We represent our neural network approximation as:\n",
    "$$u_{NN}(x; \\theta) = f_{\\theta}(x)$$\n",
    "\n",
    "where $\\theta$ represents all trainable parameters (weights and biases). The goal is to find parameters $\\theta^*$ such that:\n",
    "$$u_{NN}(x; \\theta^*) \\approx u^*(x) \\quad \\forall x \\in [0,1]$$\n",
    "\n",
    "We achieve this by minimizing a loss function over a training dataset $\\{(x_i, u_i)\\}_{i=1}^N$:\n",
    "$$\\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\left(u_{NN}(x_i; \\theta) - u_i\\right)^2$$\n",
    "\n",
    "This framework transforms function approximation into an optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "\n",
    "# Set up reproducibility and plotting\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'figure.figsize': (12, 8),\n",
    "    'lines.linewidth': 2.5,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3\n",
    "})\n",
    "\n",
    "# Define the target function\n",
    "def target_function(x):\n",
    "    \"\"\"Target function: u(x) = sin(π*x)\"\"\"\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "# Generate training data\n",
    "n_train = 15\n",
    "x_train = np.linspace(0, 1, n_train)\n",
    "u_train = target_function(x_train)\n",
    "\n",
    "# Add small amount of noise for realism\n",
    "noise_level = 0.01\n",
    "u_train_noisy = u_train + noise_level * np.random.randn(n_train)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train.reshape(-1, 1), dtype=torch.float32)\n",
    "u_train_tensor = torch.tensor(u_train_noisy.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "print(f\"Training dataset: {n_train} points\")\n",
    "print(f\"Input range: [{x_train.min():.3f}, {x_train.max():.3f}]\")\n",
    "print(f\"Target range: [{u_train.min():.3f}, {u_train.max():.3f}]\")\n",
    "print(f\"Noise level: {noise_level}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Neuron Approximation\n",
    "\n",
    "We begin with the simplest possible neural network: a single neuron with one input and one output. This provides crucial insight into the fundamental mechanics of neural network learning.\n",
    "\n",
    "The single neuron computes:\n",
    "$$u_{NN}(x) = \\sigma(w \\cdot x + b)$$\n",
    "\n",
    "where $\\sigma$ is the activation function, $w$ is the weight, and $b$ is the bias.\n",
    "\n",
    "**Question**: Can a single neuron approximate $\\sin(\\pi x)$? What are the limitations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleNeuron(nn.Module):\n",
    "    \"\"\"A single neuron with customizable activation function\"\"\"\n",
    "    \n",
    "    def __init__(self, activation='tanh'):\n",
    "        super(SingleNeuron, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        else:\n",
    "            self.activation = nn.Identity()  # Linear activation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.activation(self.linear(x))\n",
    "\n",
    "def train_network(model, x_train, u_train, epochs=5000, lr=0.01, verbose=False):\n",
    "    \"\"\"Train a neural network model\"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        predictions = model(x_train)\n",
    "        loss = criterion(predictions, u_train)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if verbose and (epoch + 1) % 1000 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train single neurons with different activation functions\n",
    "activations = ['linear', 'tanh', 'sigmoid', 'relu']\n",
    "models = {}\n",
    "losses_history = {}\n",
    "\n",
    "for activation in activations:\n",
    "    print(f\"\\nTraining single neuron with {activation} activation...\")\n",
    "    model = SingleNeuron(activation)\n",
    "    losses = train_network(model, x_train_tensor, u_train_tensor, epochs=5000, lr=0.01)\n",
    "    models[activation] = model\n",
    "    losses_history[activation] = losses\n",
    "    print(f\"Final loss: {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize single neuron approximations\n",
    "x_test = np.linspace(0, 1, 200)\n",
    "x_test_tensor = torch.tensor(x_test.reshape(-1, 1), dtype=torch.float32)\n",
    "u_true = target_function(x_test)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = [ax1, ax2, ax3, ax4]\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "\n",
    "for i, (activation, color) in enumerate(zip(activations, colors)):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        u_pred = models[activation](x_test_tensor).numpy().flatten()\n",
    "    \n",
    "    # Plot true function\n",
    "    ax.plot(x_test, u_true, 'k-', linewidth=3, label='True: $\\sin(\\pi x)$', alpha=0.8)\n",
    "    \n",
    "    # Plot prediction\n",
    "    ax.plot(x_test, u_pred, color=color, linewidth=2.5, linestyle='--', \n",
    "            label=f'Single Neuron ({activation.title()})')\n",
    "    \n",
    "    # Plot training data\n",
    "    ax.scatter(x_train, u_train_noisy, color='black', s=40, alpha=0.7, \n",
    "              label='Training Data', zorder=5)\n",
    "    \n",
    "    # Calculate and display error\n",
    "    mse = np.mean((u_pred - u_true)**2)\n",
    "    ax.text(0.05, 0.95, f'MSE: {mse:.4f}', transform=ax.transAxes, \n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "            verticalalignment='top', fontsize=11)\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('u(x)')\n",
    "    ax.set_title(f'Single Neuron: {activation.title()} Activation', fontweight='bold')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-0.2, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print parameter analysis\n",
    "print(\"\\nLearned Parameters:\")\n",
    "for activation in activations:\n",
    "    model = models[activation]\n",
    "    weight = model.linear.weight.item()\n",
    "    bias = model.linear.bias.item()\n",
    "    print(f\"{activation.title():>8}: w = {weight:7.3f}, b = {bias:7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer with Multiple Neurons\n",
    "\n",
    "The single neuron clearly struggles to capture the sinusoidal behavior. Let's expand to a single layer with multiple neurons:\n",
    "\n",
    "$$u_{NN}(x) = \\sum_{i=1}^{n} w_i^{(2)} \\sigma(w_i^{(1)} x + b_i^{(1)}) + b^{(2)}$$\n",
    "\n",
    "This architecture can potentially represent more complex functions by combining multiple simple transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayerNN(nn.Module):\n",
    "    \"\"\"Single hidden layer neural network\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size=10, activation='tanh'):\n",
    "        super(SingleLayerNN, self).__init__()\n",
    "        self.hidden = nn.Linear(1, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        if activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Train networks with varying hidden layer sizes\n",
    "hidden_sizes = [5, 10, 20, 50]\n",
    "activation = 'tanh'  # Best performing from single neuron experiment\n",
    "\n",
    "single_layer_models = {}\n",
    "single_layer_losses = {}\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    print(f\"\\nTraining single layer network with {hidden_size} neurons...\")\n",
    "    model = SingleLayerNN(hidden_size, activation)\n",
    "    losses = train_network(model, x_train_tensor, u_train_tensor, \n",
    "                          epochs=8000, lr=0.01, verbose=False)\n",
    "    single_layer_models[hidden_size] = model\n",
    "    single_layer_losses[hidden_size] = losses\n",
    "    print(f\"Final loss: {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of hidden layer size\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = [ax1, ax2, ax3, ax4]\n",
    "\n",
    "for i, hidden_size in enumerate(hidden_sizes):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        u_pred = single_layer_models[hidden_size](x_test_tensor).numpy().flatten()\n",
    "    \n",
    "    # Plot\n",
    "    ax.plot(x_test, u_true, 'k-', linewidth=3, label='True Function', alpha=0.8)\n",
    "    ax.plot(x_test, u_pred, 'b-', linewidth=2.5, \n",
    "            label=f'NN ({hidden_size} neurons)')\n",
    "    ax.scatter(x_train, u_train_noisy, color='red', s=40, alpha=0.7, \n",
    "              label='Training Data', zorder=5)\n",
    "    \n",
    "    # Calculate error\n",
    "    mse = np.mean((u_pred - u_true)**2)\n",
    "    max_error = np.max(np.abs(u_pred - u_true))\n",
    "    \n",
    "    ax.text(0.05, 0.95, f'MSE: {mse:.6f}\\nMax Error: {max_error:.4f}', \n",
    "            transform=ax.transAxes, \n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n",
    "            verticalalignment='top', fontsize=10)\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('u(x)')\n",
    "    ax.set_title(f'Single Layer: {hidden_size} Neurons', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-0.2, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot training convergence\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for hidden_size in hidden_sizes:\n",
    "    losses = single_layer_losses[hidden_size]\n",
    "    plt.semilogy(losses, label=f'{hidden_size} neurons', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss (log scale)')\n",
    "plt.title('Training Convergence', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "final_losses = [single_layer_losses[h][-1] for h in hidden_sizes]\n",
    "plt.loglog(hidden_sizes, final_losses, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Neurons')\n",
    "plt.ylabel('Final Loss (log scale)')\n",
    "plt.title('Loss vs Network Width', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Loss Summary:\")\n",
    "for hidden_size in hidden_sizes:\n",
    "    loss = single_layer_losses[hidden_size][-1]\n",
    "    print(f\"{hidden_size:2d} neurons: {loss:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Results\n",
    "\n",
    "Our experiments reveal several crucial insights:\n",
    "\n",
    "### 1. **Single Neuron Limitations**\n",
    "- **Linear activation**: Cannot capture any nonlinearity\n",
    "- **Nonlinear activations**: Can only represent simple curved functions\n",
    "- **Fundamental constraint**: One neuron = one \"feature\" or \"basis function\"\n",
    "\n",
    "### 2. **Width Benefits**\n",
    "- **More neurons = more flexibility**: Additional neurons provide more basis functions\n",
    "- **Diminishing returns**: Beyond a certain width, improvements become marginal\n",
    "- **Approximation quality**: Even with 50 neurons, some residual error remains\n",
    "\n",
    "### 3. **Training Dynamics**\n",
    "- **Convergence speed**: Wider networks often converge faster initially\n",
    "- **Optimization landscape**: More parameters can lead to better local minima\n",
    "- **Overfitting risk**: Very wide networks may memorize training data\n",
    "\n",
    "Let's examine these insights more deeply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis: Error distribution and feature visualization\n",
    "best_model = single_layer_models[50]  # Best performing model\n",
    "\n",
    "# Generate dense test set for error analysis\n",
    "x_dense = np.linspace(0, 1, 1000)\n",
    "x_dense_tensor = torch.tensor(x_dense.reshape(-1, 1), dtype=torch.float32)\n",
    "u_true_dense = target_function(x_dense)\n",
    "\n",
    "with torch.no_grad():\n",
    "    u_pred_dense = best_model(x_dense_tensor).numpy().flatten()\n",
    "\n",
    "# Calculate pointwise error\n",
    "error = u_pred_dense - u_true_dense\n",
    "abs_error = np.abs(error)\n",
    "\n",
    "# Create comprehensive analysis plot\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Function and approximation\n",
    "ax1.plot(x_dense, u_true_dense, 'k-', linewidth=3, label='True Function', alpha=0.8)\n",
    "ax1.plot(x_dense, u_pred_dense, 'b-', linewidth=2, label='NN Approximation')\n",
    "ax1.scatter(x_train, u_train_noisy, color='red', s=50, alpha=0.8, \n",
    "           label='Training Data', zorder=5)\n",
    "ax1.fill_between(x_dense, u_true_dense, u_pred_dense, alpha=0.3, color='red', \n",
    "                label='Approximation Error')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('u(x)')\n",
    "ax1.set_title('Best Single Layer Approximation (50 neurons)', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Error analysis\n",
    "ax2.plot(x_dense, error, 'r-', linewidth=2, label='Signed Error')\n",
    "ax2.plot(x_dense, abs_error, 'orange', linewidth=2, label='Absolute Error')\n",
    "ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax2.fill_between(x_dense, 0, error, alpha=0.3, color='red')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('Error')\n",
    "ax2.set_title('Pointwise Error Analysis', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Error statistics\n",
    "error_metrics = {\n",
    "    'MSE': np.mean(error**2),\n",
    "    'MAE': np.mean(abs_error),\n",
    "    'Max Error': np.max(abs_error),\n",
    "    'Std Error': np.std(error)\n",
    "}\n",
    "\n",
    "metrics_names = list(error_metrics.keys())\n",
    "metrics_values = list(error_metrics.values())\n",
    "\n",
    "bars = ax3.bar(metrics_names, metrics_values, color=['blue', 'green', 'red', 'orange'])\n",
    "ax3.set_ylabel('Error Value')\n",
    "ax3.set_title('Error Metrics Summary', fontweight='bold')\n",
    "ax3.set_yscale('log')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, metrics_values):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{value:.2e}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 4: Hidden layer features\n",
    "# Extract and visualize what the hidden neurons learned\n",
    "with torch.no_grad():\n",
    "    hidden_output = best_model.activation(best_model.hidden(x_dense_tensor))\n",
    "    hidden_features = hidden_output.numpy()\n",
    "\n",
    "# Plot first 5 hidden features\n",
    "for i in range(min(5, hidden_features.shape[1])):\n",
    "    ax4.plot(x_dense, hidden_features[:, i], alpha=0.7, linewidth=1.5, \n",
    "             label=f'Neuron {i+1}')\n",
    "\n",
    "ax4.set_xlabel('x')\n",
    "ax4.set_ylabel('Activation')\n",
    "ax4.set_title('Hidden Layer Features (First 5 Neurons)', fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDetailed Error Analysis:\")\n",
    "for metric, value in error_metrics.items():\n",
    "    print(f\"{metric:>12}: {value:.6e}\")\n",
    "\n",
    "print(f\"\\nError Statistics:\")\n",
    "print(f\"Mean error: {np.mean(error):.6e}\")\n",
    "print(f\"Error range: [{np.min(error):.6e}, {np.max(error):.6e}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights and Implications\n",
    "\n",
    "This systematic exploration of neural network function approximation reveals fundamental principles:\n",
    "\n",
    "### **Universal Approximation Intuition**\n",
    "The improvement with increased width hints at the **Universal Approximation Theorem**: a single hidden layer with sufficient neurons can approximate any continuous function to arbitrary accuracy. However, \"sufficient\" may require impractically many neurons.\n",
    "\n",
    "### **The Role of Activation Functions**\n",
    "Nonlinear activations are essential. Without them, the network reduces to linear regression, regardless of depth or width. The choice of activation function affects:\n",
    "- **Approximation quality**: Tanh and sigmoid performed better than ReLU for this smooth function\n",
    "- **Training dynamics**: Different activations have different gradient properties\n",
    "- **Representation capacity**: Each activation function creates different \"basis functions\"\n",
    "\n",
    "### **Width vs. Approximation Quality**\n",
    "More neurons generally improve approximation, but with diminishing returns. This suggests that width alone may not be the most efficient path to better approximation.\n",
    "\n",
    "### **Residual Error Patterns**\n",
    "Even our best single-layer network shows systematic errors. The error is not random—it has structure, suggesting that the network is missing certain aspects of the target function that a single layer cannot capture.\n",
    "\n",
    "## The Path Forward\n",
    "\n",
    "Our experiments raise crucial questions:\n",
    "\n",
    "1. **How wide is \"wide enough\"?** What determines the minimum width needed for a given approximation accuracy?\n",
    "\n",
    "2. **Can we do better with depth?** Would multiple layers provide more efficient approximation than width alone?\n",
    "\n",
    "3. **What about more complex functions?** How do these results extend to functions with higher frequency content or discontinuities?\n",
    "\n",
    "4. **What theoretical guarantees exist?** Can we predict approximation quality without trial-and-error?\n",
    "\n",
    "These questions motivate our subsequent investigations into network architecture, universal approximation theory, and the fundamental limitations that drive the need for deeper networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Preview\n",
    "\n",
    "In this notebook, we established the practical foundation of neural network function approximation:\n",
    "\n",
    "1. **Mathematical Framework**: Formulation of function approximation as optimization\n",
    "2. **Single Neuron Analysis**: Fundamental limitations of minimal architectures\n",
    "3. **Width Effects**: How additional neurons improve approximation capacity\n",
    "4. **Activation Function Importance**: The critical role of nonlinearity\n",
    "5. **Performance Analysis**: Systematic evaluation of approximation quality\n",
    "\n",
    "**Key Finding**: Single-layer networks can approximate smooth functions reasonably well, but require many neurons and still exhibit systematic errors.\n",
    "\n",
    "**Next Steps**: Our subsequent notebooks will explore:\n",
    "- **Width sufficiency**: Theoretical and practical limits of single-layer approximation\n",
    "- **Nonlinearity deep-dive**: Why activation functions are mathematically necessary\n",
    "- **Universal approximation**: The theoretical guarantees and their practical implications\n",
    "- **Failure modes**: Problems that expose single-layer limitations\n",
    "- **Depth advantages**: How multiple layers overcome these limitations\n",
    "\n",
    "The journey from function approximation to deep learning continues with deeper theoretical understanding and more challenging problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
