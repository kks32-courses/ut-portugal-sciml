{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR Problem and the Need for Depth\n",
    "\n",
    "## The XOR Problem: Historical Context\n",
    "\n",
    "The XOR (exclusive OR) problem exposed fundamental limitations of **true** single-layer perceptrons, leading to the \"AI winter\" of the 1970s after Minsky and Papert's critique. This simple problem reveals why depth is essential in neural networks.\n",
    "\n",
    "**XOR Truth Table:**\n",
    "```\n",
    "x₁  x₂  │  y\n",
    "────────┼────\n",
    " 0   0  │  0\n",
    " 0   1  │  1  \n",
    " 1   0  │  1\n",
    " 1   1  │  0\n",
    "```\n",
    "\n",
    "**Key insight**: XOR is not linearly separable - no single line can separate the classes.\n",
    "\n",
    "**Historical Note**: A true single-layer perceptron has NO hidden layers - just direct input-to-output connections. This is what Minsky and Papert showed cannot solve XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({'font.size': 12, 'figure.figsize': (14, 10)})\n",
    "\n",
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_xor = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X_xor_tensor = torch.tensor(X_xor)\n",
    "y_xor_tensor = torch.tensor(y_xor)\n",
    "\n",
    "print(\"XOR Dataset:\")\n",
    "for i in range(4):\n",
    "    print(f\"({X_xor[i,0]}, {X_xor[i,1]}) → {y_xor[i,0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Single Layer Fails: Geometric Intuition\n",
    "\n",
    "A **true** single-layer perceptron can only create linear decision boundaries. The XOR problem requires a non-linear boundary that no single line can provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize XOR problem\n",
    "def plot_xor_problem():\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: XOR data points with failed linear separation attempts\n",
    "    colors = ['red', 'blue']\n",
    "    labels = ['Class 0', 'Class 1']\n",
    "    \n",
    "    for i in range(2):\n",
    "        mask = y_xor.flatten() == i\n",
    "        ax1.scatter(X_xor[mask, 0], X_xor[mask, 1], \n",
    "                   c=colors[i], s=200, alpha=0.8, \n",
    "                   label=labels[i], edgecolors='black', linewidth=2)\n",
    "    \n",
    "    # Try to draw separating lines (all fail)\n",
    "    x_line = np.linspace(-0.5, 1.5, 100)\n",
    "    ax1.plot(x_line, 0.5 * np.ones_like(x_line), 'g--', linewidth=2, alpha=0.7, label='Failed Attempt 1')\n",
    "    ax1.plot(0.5 * np.ones_like(x_line), x_line, 'm--', linewidth=2, alpha=0.7, label='Failed Attempt 2')\n",
    "    ax1.plot(x_line, x_line, 'orange', linestyle='--', linewidth=2, alpha=0.7, label='Failed Attempt 3')\n",
    "    \n",
    "    ax1.set_xlim(-0.3, 1.3)\n",
    "    ax1.set_ylim(-0.3, 1.3)\n",
    "    ax1.set_xlabel('x₁')\n",
    "    ax1.set_ylabel('x₂')\n",
    "    ax1.set_title('XOR Problem: No Linear Separation Possible', fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Required non-linear boundary\n",
    "    for i in range(2):\n",
    "        mask = y_xor.flatten() == i\n",
    "        ax2.scatter(X_xor[mask, 0], X_xor[mask, 1], \n",
    "                   c=colors[i], s=200, alpha=0.8, \n",
    "                   label=labels[i], edgecolors='black', linewidth=2)\n",
    "    \n",
    "    # Draw the required non-linear boundary (conceptual)\n",
    "    theta = np.linspace(0, 2*np.pi, 100)\n",
    "    x_circle1 = 0.25 + 0.15*np.cos(theta)\n",
    "    y_circle1 = 0.25 + 0.15*np.sin(theta)\n",
    "    x_circle2 = 0.75 + 0.15*np.cos(theta)\n",
    "    y_circle2 = 0.75 + 0.15*np.sin(theta)\n",
    "    \n",
    "    ax2.plot(x_circle1, y_circle1, 'g-', linewidth=3, alpha=0.8, label='Required Non-Linear Boundary')\n",
    "    ax2.plot(x_circle2, y_circle2, 'g-', linewidth=3, alpha=0.8)\n",
    "    \n",
    "    ax2.set_xlim(-0.3, 1.3)\n",
    "    ax2.set_ylim(-0.3, 1.3)\n",
    "    ax2.set_xlabel('x₁')\n",
    "    ax2.set_ylabel('x₂')\n",
    "    ax2.set_title('Required Non-Linear Boundary', fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_xor_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architectures: The Critical Difference\n",
    "\n",
    "Let's define the architectures correctly to demonstrate the historical problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRUE Single Layer Perceptron (what Minsky & Papert analyzed)\n",
    "class TrueSingleLayerPerceptron(nn.Module):\n",
    "    \"\"\"A TRUE single-layer perceptron with NO hidden layers.\n",
    "    Architecture: Input → Output (direct connection)\n",
    "    This is what cannot solve XOR!\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Direct connection from 2 inputs to 1 output - NO hidden layer\n",
    "        self.layer = nn.Linear(2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.layer(x))\n",
    "    \n",
    "    def get_weights(self):\n",
    "        \"\"\"Return weights for analysis\"\"\"\n",
    "        return self.layer.weight.data, self.layer.bias.data\n",
    "\n",
    "# Multi-Layer Perceptron (what actually solves XOR)\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    \"\"\"A multi-layer perceptron with one hidden layer.\n",
    "    Architecture: Input → Hidden → Output\n",
    "    This CAN solve XOR!\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size=4):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(2, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.output(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "def train_xor_model(model, X, y, epochs=5000, lr=1.0):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        pred = model(X)\n",
    "        loss = criterion(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            accuracy = ((pred > 0.5).float() == y).float().mean()\n",
    "            print(f'Epoch {epoch+1}: Loss = {loss.item():.4f}, Accuracy = {accuracy:.4f}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "print(\"Network architectures defined:\")\n",
    "print(\"1. TrueSingleLayerPerceptron: Input → Output (NO hidden layer)\")\n",
    "print(\"2. MultiLayerPerceptron: Input → Hidden → Output (1 hidden layer)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating the Historical Problem\n",
    "\n",
    "Now let's train the TRUE single-layer perceptron and watch it fail to solve XOR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train TRUE single layer perceptron (this SHOULD and WILL fail!)\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING TRUE SINGLE LAYER PERCEPTRON (Input → Output)\")\n",
    "print(\"This is what Minsky & Papert showed cannot solve XOR!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "true_single = TrueSingleLayerPerceptron()\n",
    "\n",
    "print(f\"Initial weights: {true_single.get_weights()}\")\n",
    "print(\"\\nTraining...\")\n",
    "\n",
    "true_single_losses = train_xor_model(true_single, X_xor_tensor, y_xor_tensor, lr=10.0)\n",
    "\n",
    "# Analyze the failure\n",
    "with torch.no_grad():\n",
    "    pred = true_single(X_xor_tensor)\n",
    "    accuracy = ((pred > 0.5).float() == y_xor_tensor).float().mean()\n",
    "    \n",
    "    print(f\"\\nFINAL RESULTS:\")\n",
    "    print(f\"Final accuracy: {accuracy:.4f} (should be around 0.5 - random guessing!)\")\n",
    "    print(f\"Final weights: {true_single.get_weights()}\")\n",
    "    print(\"\\nPredictions vs Targets:\")\n",
    "    for i in range(4):\n",
    "        print(f\"  Input: ({X_xor[i,0]}, {X_xor[i,1]}) → Prediction: {pred[i,0]:.4f}, Target: {y_xor[i,0]}\")\n",
    "    \n",
    "    print(\"\\n❌ CONCLUSION: The TRUE single-layer perceptron FAILS at XOR!\")\n",
    "    print(\"   This demonstrates Minsky & Papert's historical critique.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Solution: Adding Hidden Layers\n",
    "\n",
    "Now let's see how adding even a single hidden layer solves the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multi-layer perceptron (this should succeed)\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING MULTI-LAYER PERCEPTRON (Input → Hidden → Output)\")\n",
    "print(\"Adding just ONE hidden layer should solve XOR!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "multi_layer = MultiLayerPerceptron(4)\n",
    "multi_layer_losses = train_xor_model(multi_layer, X_xor_tensor, y_xor_tensor, lr=10.0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = multi_layer(X_xor_tensor)\n",
    "    accuracy = ((pred > 0.5).float() == y_xor_tensor).float().mean()\n",
    "    \n",
    "    print(f\"\\nFINAL RESULTS:\")\n",
    "    print(f\"Final accuracy: {accuracy:.4f} (should be 1.0000!)\")\n",
    "    print(\"\\nPredictions vs Targets:\")\n",
    "    for i in range(4):\n",
    "        print(f\"  Input: ({X_xor[i,0]}, {X_xor[i,1]}) → Prediction: {pred[i,0]:.4f}, Target: {y_xor[i,0]}\")\n",
    "    \n",
    "    print(\"\\n✅ SUCCESS: The multi-layer perceptron SOLVES XOR!\")\n",
    "    print(\"   Adding hidden layers enables non-linear transformations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Decision Boundaries\n",
    "\n",
    "Let's see the geometric difference between linear and non-linear decision boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundaries\n",
    "def plot_decision_boundary(model, title, ax):\n",
    "    h = 0.01\n",
    "    x_min, x_max = -0.5, 1.5\n",
    "    y_min, y_max = -0.5, 1.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    grid_points = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        Z = model(grid_points).numpy()\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Create contour plot\n",
    "    contour = ax.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap='RdYlBu')\n",
    "    ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=3, linestyles='-')\n",
    "    \n",
    "    # Plot XOR points\n",
    "    colors = ['red', 'blue']\n",
    "    markers = ['o', 's']\n",
    "    for i in range(2):\n",
    "        mask = y_xor.flatten() == i\n",
    "        ax.scatter(X_xor[mask, 0], X_xor[mask, 1], \n",
    "                  c=colors[i], s=300, alpha=1.0, \n",
    "                  edgecolors='black', linewidth=3,\n",
    "                  marker=markers[i], label=f'Class {i}')\n",
    "    \n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_title(title, fontweight='bold', fontsize=14)\n",
    "    ax.set_xlabel('x₁', fontsize=12)\n",
    "    ax.set_ylabel('x₂', fontsize=12)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Create the comparison plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# True single layer (should show linear boundary)\n",
    "plot_decision_boundary(true_single, 'TRUE Single Layer\\n(Linear Boundary)\\n FAILS', ax1)\n",
    "\n",
    "# Multi-layer (should solve XOR)\n",
    "plot_decision_boundary(multi_layer, 'Multi-Layer\\n(Non-Linear Boundary)\\n SUCCEEDS', ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"OBSERVATION:\")\n",
    "print(\"• TRUE Single Layer: Can only create a straight line (linear boundary)\")\n",
    "print(\"• Multi-Layer: Creates curved boundaries that can separate XOR classes\")\n",
    "print(\"• Deep Network: Can create even more complex boundaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Explanation: Why Depth Solves XOR\n",
    "\n",
    "### The Problem with True Single-Layer Perceptrons\n",
    "\n",
    "A true single-layer perceptron computes:\n",
    "$y = \\sigma(w_1 x_1 + w_2 x_2 + b)$\n",
    "\n",
    "The decision boundary is defined by $w_1 x_1 + w_2 x_2 + b = 0$, which is always a **straight line**. \n",
    "\n",
    "For XOR, we need to separate:\n",
    "- Class 0: (0,0) and (1,1) \n",
    "- Class 1: (0,1) and (1,0)\n",
    "\n",
    "No single straight line can accomplish this separation!\n",
    "\n",
    "### The Solution: Multi-Layer Networks\n",
    "\n",
    "A multi-layer network can solve XOR by decomposing it into simpler problems:\n",
    "\n",
    "**Layer 1 (Hidden)**: Create intermediate features\n",
    "- $h_1 = \\sigma(w_{11} x_1 + w_{12} x_2 + b_1)$ ≈ $x_1$ OR $x_2$\n",
    "- $h_2 = \\sigma(w_{21} x_1 + w_{22} x_2 + b_2)$ ≈ $x_1$ AND $x_2$\n",
    "\n",
    "**Layer 2 (Output)**: Combine features\n",
    "- $y = \\sigma(v_1 h_1 + v_2 h_2 + b_3)$ ≈ OR AND NOT = XOR\n",
    "\n",
    "This demonstrates the **compositional power** of depth: complex functions can be built from simpler components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond XOR: High-Frequency Functions\n",
    "\n",
    "The limitations of shallow networks extend beyond simple classification. Let's see how they struggle with high-frequency functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-frequency function demonstration\n",
    "def high_freq_function(x):\n",
    "    return np.sin(np.pi * x) + 0.3 * np.sin(10 * np.pi * x)\n",
    "\n",
    "x_hf = np.linspace(0, 1, 100)\n",
    "y_hf_true = high_freq_function(x_hf)\n",
    "\n",
    "# Training data (sparse sampling)\n",
    "np.random.seed(42)\n",
    "x_hf_train = np.linspace(0, 1, 25)\n",
    "y_hf_train = high_freq_function(x_hf_train) + 0.01 * np.random.randn(25)\n",
    "\n",
    "x_hf_train_t = torch.tensor(x_hf_train.reshape(-1, 1), dtype=torch.float32)\n",
    "y_hf_train_t = torch.tensor(y_hf_train.reshape(-1, 1), dtype=torch.float32)\n",
    "x_hf_test_t = torch.tensor(x_hf.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# Network architectures for regression\n",
    "class ShallowRegressor(nn.Module):\n",
    "    \"\"\"Single hidden layer for regression\"\"\"\n",
    "    def __init__(self, width=50):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(1, width),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(width, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class DeepRegressor(nn.Module):\n",
    "    \"\"\"Multiple hidden layers for regression\"\"\"\n",
    "    def __init__(self, width=20, depth=3):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(1, width), nn.Tanh()]\n",
    "        for _ in range(depth-1):\n",
    "            layers.extend([nn.Linear(width, width), nn.Tanh()])\n",
    "        layers.append(nn.Linear(width, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def train_regressor(model, x_train, y_train, epochs=3000, lr=0.01):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        pred = model(x_train)\n",
    "        loss = criterion(pred, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "# Train models\n",
    "print(\"Training regressors on high-frequency function...\")\n",
    "torch.manual_seed(42)\n",
    "shallow_reg = ShallowRegressor(100)  # More neurons to be fair\n",
    "deep_reg = DeepRegressor(20, 4)      # Fewer neurons but more layers\n",
    "\n",
    "shallow_loss = train_regressor(shallow_reg, x_hf_train_t, y_hf_train_t)\n",
    "deep_loss = train_regressor(deep_reg, x_hf_train_t, y_hf_train_t)\n",
    "\n",
    "print(f\"Shallow network final loss: {shallow_loss:.6f}\")\n",
    "print(f\"Deep network final loss: {deep_loss:.6f}\")\n",
    "print(f\"Improvement factor: {shallow_loss/deep_loss:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize high-frequency function results\n",
    "with torch.no_grad():\n",
    "    y_shallow_pred = shallow_reg(x_hf_test_t).numpy().flatten()\n",
    "    y_deep_pred = deep_reg(x_hf_test_t).numpy().flatten()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Shallow network result\n",
    "ax1.plot(x_hf, y_hf_true, 'k-', linewidth=3, label='True Function', alpha=0.8)\n",
    "ax1.plot(x_hf, y_shallow_pred, 'r-', linewidth=2, label='Shallow Network (100 neurons)')\n",
    "ax1.scatter(x_hf_train, y_hf_train, color='blue', s=40, alpha=0.7, zorder=5, label='Training Data')\n",
    "\n",
    "shallow_mse = np.mean((y_shallow_pred - y_hf_true)**2)\n",
    "ax1.text(0.05, 0.95, f'MSE: {shallow_mse:.4f}', transform=ax1.transAxes,\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "         fontsize=12, fontweight='bold')\n",
    "\n",
    "ax1.set_title('Shallow Network\\n(1 Hidden Layer)', fontweight='bold', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "\n",
    "# Deep network result\n",
    "ax2.plot(x_hf, y_hf_true, 'k-', linewidth=3, label='True Function', alpha=0.8)\n",
    "ax2.plot(x_hf, y_deep_pred, 'g-', linewidth=2, label='Deep Network (4 layers)')\n",
    "ax2.scatter(x_hf_train, y_hf_train, color='blue', s=40, alpha=0.7, zorder=5, label='Training Data')\n",
    "\n",
    "deep_mse = np.mean((y_deep_pred - y_hf_true)**2)\n",
    "ax2.text(0.05, 0.95, f'MSE: {deep_mse:.4f}', transform=ax2.transAxes,\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "         fontsize=12, fontweight='bold')\n",
    "\n",
    "ax2.set_title('Deep Network\\n(4 Hidden Layers)', fontweight='bold', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "\n",
    "plt.suptitle('High-Frequency Function Approximation', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nRESULTS COMPARISON:\")\n",
    "print(f\"Shallow network MSE: {shallow_mse:.6f}\")\n",
    "print(f\"Deep network MSE: {deep_mse:.6f}\")\n",
    "print(f\"Deep network is {shallow_mse/deep_mse:.1f}× better!\")\n",
    "print(\"\\nThe deep network captures high-frequency components much better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights: Why Depth Matters\n",
    "\n",
    "### 1. **Representation Efficiency**\n",
    "- **Shallow networks**: Require exponentially many neurons for complex patterns\n",
    "- **Deep networks**: Hierarchical feature composition is exponentially more efficient\n",
    "\n",
    "### 2. **Feature Hierarchy**\n",
    "- **Layer 1**: Simple features (edges, basic patterns)\n",
    "- **Layer 2**: Combinations of simple features (corners, textures)\n",
    "- **Layer 3+**: Complex abstractions (objects, concepts)\n",
    "\n",
    "### 3. **Geometric Perspective**\n",
    "- Each layer performs a **coordinate transformation**\n",
    "- Deep networks can \"unfold\" complex manifolds\n",
    "- **XOR example**: Transform non-separable data into separable representation\n",
    "\n",
    "### 4. **Practical Implications**\n",
    "- **Universal approximation ≠ efficient approximation**\n",
    "- **Depth enables compositional learning**\n",
    "- **Real-world functions often have hierarchical structure**\n",
    "\n",
    "## Historical Timeline and Impact\n",
    "\n",
    "| Year | Event | Impact |\n",
    "|------|-------|--------|\n",
    "| 1943 | McCulloch-Pitts neuron | Foundation of neural networks |\n",
    "| 1957 | Rosenblatt's Perceptron | First learning algorithm |\n",
    "| **1969** | **Minsky & Papert critique** | **Showed single-layer limitations (XOR)** |\n",
    "| 1970s-80s | \"AI Winter\" | Reduced funding and interest |\n",
    "| 1986 | Backpropagation | Enabled training of multi-layer networks |\n",
    "| 2006+ | Deep Learning Revolution | Showed power of very deep networks |\n",
    "\n",
    "## Summary\n",
    "\n",
    "The XOR problem reveals fundamental limitations of **true** single-layer perceptrons:\n",
    "\n",
    "### ❌ **Single-Layer Limitations**:\n",
    "1. **Linear separability constraint**: Can only create straight-line boundaries\n",
    "2. **Cannot solve XOR**: Accuracy stuck around 50% (random guessing)\n",
    "3. **Limited expressiveness**: Cannot represent complex functions efficiently\n",
    "\n",
    "### ✅ **Multi-Layer Solutions**:\n",
    "1. **Non-linear transformations**: Hidden layers enable curved boundaries\n",
    "2. **Solves XOR perfectly**: 100% accuracy achievable\n",
    "3. **Hierarchical representations**: Complex patterns from simple building blocks\n",
    "4. **Efficiency gains**: Often requires fewer total parameters than wide shallow networks\n",
    "\n",
    "### 🌟 **Modern Relevance**:\n",
    "**Historical impact**: Understanding these limitations led to the deep learning revolution, showing that depth is not just helpful but **essential** for many real-world problems.\n",
    "\n",
    "**Today's applications**: These principles scale to:\n",
    "- **Computer Vision**: Hierarchical feature detection (edges → shapes → objects)\n",
    "- **Natural Language Processing**: From characters → words → sentences → meaning\n",
    "- **Scientific Computing**: Multi-scale physical phenomena\n",
    "- **Anywhere hierarchical pattern recognition is needed**\n",
    "\n",
    "The XOR problem, while simple, encapsulates one of the most important insights in machine learning: **depth transforms impossible problems into solvable ones**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
