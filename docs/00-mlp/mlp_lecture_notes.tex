\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{listings}
\usepackage{xcolor}

\geometry{a4paper, margin=1in}
\pagestyle{fancy}
\fancyhf{}
\rhead{CE397 - Scientific Machine Learning}
\lhead{Multi-Layer Perceptron}
\cfoot{\thepage}

\title{Multi-Layer Perceptron (MLP) for Scientific Machine Learning}
\author{Instructor: Krishna Kumar}
\date{}

\begin{document}

\maketitle

\section{Introduction}

The Multi-Layer Perceptron (MLP) is a fundamental building block in neural networks and Scientific Machine Learning (SciML). Unlike the simple perceptron, which can only learn linearly separable patterns, MLPs can approximate complex non-linear functions through the composition of multiple layers and non-linear activation functions.

\section{From Perceptron to MLP}

\subsection{The Perceptron}

A perceptron is a simple single-layer neural network that computes:

\begin{align}
z &= w_0 + \sum_{i=1}^{n} w_i x_i = w_0 + \boldsymbol{w}^T\boldsymbol{x} \label{eq:perceptron_linear}\\
\hat{y} &= g(z) \label{eq:perceptron_activation}
\end{align}

where:
\begin{itemize}
    \item $\boldsymbol{x} = [x_1, x_2, \ldots, x_n]^T$ is the input vector
    \item $\boldsymbol{w} = [w_1, w_2, \ldots, w_n]^T$ is the weight vector
    \item $w_0$ is the bias term
    \item $g(\cdot)$ is the activation function
    \item $z$ is the pre-activation value
    \item $\hat{y}$ is the predicted output
\end{itemize}

\subsection{Multi-Output Perceptrons}

For multiple outputs, we use multiple perceptrons sharing the same inputs:

\begin{align}
z_i &= b_i + \sum_{j=1}^{m} w_{j,i} x_j \quad \text{for } i = 1, 2, \ldots, k \\
\hat{y}_i &= g(z_i)
\end{align}

This can be written in matrix form as:
\begin{align}
\boldsymbol{z} &= \boldsymbol{b} + \boldsymbol{W}^T\boldsymbol{x} \\
\hat{\boldsymbol{y}} &= g(\boldsymbol{z})
\end{align}

where $\boldsymbol{W} \in \mathbb{R}^{m \times k}$ is the weight matrix and $\boldsymbol{b} \in \mathbb{R}^{k}$ is the bias vector.

\section{Multi-Layer Perceptron Architecture}

\subsection{Hidden Layers}

The key innovation in MLPs is the introduction of hidden layers between the input and output layers. For a single hidden layer MLP:

\begin{align}
\boldsymbol{z}^{(1)} &= \boldsymbol{b}^{(1)} + \boldsymbol{W}^{(1)T}\boldsymbol{x} \label{eq:hidden_linear}\\
\boldsymbol{a}^{(1)} &= g(\boldsymbol{z}^{(1)}) \label{eq:hidden_activation}\\
\boldsymbol{z}^{(2)} &= \boldsymbol{b}^{(2)} + \boldsymbol{W}^{(2)T}\boldsymbol{a}^{(1)} \label{eq:output_linear}\\
\hat{\boldsymbol{y}} &= f(\boldsymbol{z}^{(2)}) \label{eq:output_activation}
\end{align}

where:
\begin{itemize}
    \item $\boldsymbol{W}^{(1)} \in \mathbb{R}^{n \times h}$ is the input-to-hidden weight matrix
    \item $\boldsymbol{b}^{(1)} \in \mathbb{R}^{h}$ is the hidden layer bias vector
    \item $\boldsymbol{W}^{(2)} \in \mathbb{R}^{h \times k}$ is the hidden-to-output weight matrix
    \item $\boldsymbol{b}^{(2)} \in \mathbb{R}^{k}$ is the output layer bias vector
    \item $h$ is the number of hidden units
    \item $g(\cdot)$ is the hidden layer activation function
    \item $f(\cdot)$ is the output layer activation function
\end{itemize}

\subsection{Deep MLPs}

For deeper networks with $L$ layers:

\begin{align}
\boldsymbol{z}^{(1)} &= \boldsymbol{b}^{(1)} + \boldsymbol{W}^{(1)T}\boldsymbol{x} \\
\boldsymbol{a}^{(1)} &= g(\boldsymbol{z}^{(1)}) \\
\boldsymbol{z}^{(l)} &= \boldsymbol{b}^{(l)} + \boldsymbol{W}^{(l)T}\boldsymbol{a}^{(l-1)} \quad \text{for } l = 2, 3, \ldots, L\\
\boldsymbol{a}^{(l)} &= g(\boldsymbol{z}^{(l)}) \quad \text{for } l = 2, 3, \ldots, L-1\\
\hat{\boldsymbol{y}} &= f(\boldsymbol{z}^{(L)})
\end{align}

\section{Activation Functions}

\subsection{Sigmoid}

The sigmoid function squashes inputs to the range $(0, 1)$:

\begin{align}
\sigma(x) &= \frac{1}{1 + e^{-x}} \\
\frac{d\sigma}{dx} &= \sigma(x)(1 - \sigma(x))
\end{align}

\textbf{Properties:}
\begin{itemize}
    \item Output range: $(0, 1)$
    \item Differentiable everywhere
    \item Saturates for large $|x|$ (vanishing gradient problem)
    \item Non-zero centered
\end{itemize}

\subsection{Hyperbolic Tangent (Tanh)}

The tanh function is a scaled and shifted sigmoid:

\begin{align}
\tanh(x) &= \frac{e^x - e^{-x}}{e^x + e^{-x}} = 2\sigma(2x) - 1 \\
\frac{d\tanh}{dx} &= 1 - \tanh^2(x)
\end{align}

\textbf{Properties:}
\begin{itemize}
    \item Output range: $(-1, 1)$
    \item Zero-centered
    \item Still suffers from vanishing gradients
\end{itemize}

\subsection{Rectified Linear Unit (ReLU)}

The ReLU function is defined as:

\begin{align}
\text{ReLU}(x) &= \max(0, x) \\
\frac{d\text{ReLU}}{dx} &= \begin{cases} 
1 & \text{if } x > 0 \\
0 & \text{if } x \leq 0
\end{cases}
\end{align}

\textbf{Properties:}
\begin{itemize}
    \item Output range: $[0, \infty)$
    \item Computationally efficient
    \item Alleviates vanishing gradient problem
    \item Can suffer from "dying ReLU" problem
\end{itemize}

\subsection{Leaky ReLU}

Addresses the dying ReLU problem:

\begin{align}
\text{LeakyReLU}(x) &= \max(\alpha x, x) \\
\frac{d\text{LeakyReLU}}{dx} &= \begin{cases} 
1 & \text{if } x > 0 \\
\alpha & \text{if } x \leq 0
\end{cases}
\end{align}

where $\alpha$ is a small positive constant (typically 0.01).

\section{Universal Approximation Theorem}

The Universal Approximation Theorem states that MLPs with:
\begin{itemize}
    \item At least one hidden layer
    \item Sufficient number of hidden units
    \item Non-linear activation functions
\end{itemize}

can approximate any continuous function on a compact subset of $\mathbb{R}^n$ to arbitrary accuracy.

\textbf{Mathematical Statement:}
Let $\phi$ be a non-constant, bounded, and continuous function. Then finite sums of the form:

\begin{equation}
G(x) = \sum_{j=1}^{N} \alpha_j \phi\left(\sum_{i=1}^{n} w_{ji} x_i + \theta_j\right)
\end{equation}

are dense in $C(I_n)$, where $I_n$ is the $n$-dimensional unit hypercube and $C(I_n)$ is the space of continuous functions on $I_n$.

\section{Training MLPs}

\subsection{Loss Functions}

\subsubsection{Mean Squared Error (MSE)}
For regression problems:

\begin{align}
\mathcal{L}(\boldsymbol{w}) &= \frac{1}{2N} \sum_{i=1}^{N} \|\boldsymbol{y}^{(i)} - \hat{\boldsymbol{y}}^{(i)}\|^2 \\
&= \frac{1}{2N} \sum_{i=1}^{N} \sum_{j=1}^{k} (y_j^{(i)} - \hat{y}_j^{(i)})^2
\end{align}

\subsubsection{Cross-Entropy Loss}
For classification problems:

\begin{align}
\mathcal{L}(\boldsymbol{w}) &= -\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{k} y_j^{(i)} \log(\hat{y}_j^{(i)})
\end{align}

\subsection{Backpropagation Algorithm}

Backpropagation computes gradients using the chain rule:

\begin{align}
\frac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}} &= \frac{\partial \mathcal{L}}{\partial z_j^{(l)}} \frac{\partial z_j^{(l)}}{\partial w_{ij}^{(l)}} \\
&= \delta_j^{(l)} a_i^{(l-1)}
\end{align}

where $\delta_j^{(l)} = \frac{\partial \mathcal{L}}{\partial z_j^{(l)}}$ is the error term for neuron $j$ in layer $l$.

\textbf{Error Propagation:}
\begin{align}
\delta_j^{(L)} &= \frac{\partial \mathcal{L}}{\partial a_j^{(L)}} \frac{\partial a_j^{(L)}}{\partial z_j^{(L)}} \\
\delta_j^{(l)} &= \left(\sum_{k} \delta_k^{(l+1)} w_{jk}^{(l+1)}\right) g'(z_j^{(l)})
\end{align}

\textbf{Parameter Updates:}
\begin{align}
w_{ij}^{(l)} &\leftarrow w_{ij}^{(l)} - \eta \frac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}} \\
b_j^{(l)} &\leftarrow b_j^{(l)} - \eta \frac{\partial \mathcal{L}}{\partial b_j^{(l)}}
\end{align}

where $\eta$ is the learning rate.

\section{Gradient Descent Optimization}

\subsection{Batch Gradient Descent}
Uses the entire dataset for each parameter update:

\begin{align}
\boldsymbol{w}^{(t+1)} &= \boldsymbol{w}^{(t)} - \eta \nabla_{\boldsymbol{w}} \mathcal{L}(\boldsymbol{w}^{(t)})
\end{align}

\subsection{Stochastic Gradient Descent (SGD)}
Uses a single sample for each update:

\begin{align}
\boldsymbol{w}^{(t+1)} &= \boldsymbol{w}^{(t)} - \eta \nabla_{\boldsymbol{w}} \mathcal{L}^{(i)}(\boldsymbol{w}^{(t)})
\end{align}

\subsection{Mini-batch Gradient Descent}
Uses a small batch of samples:

\begin{align}
\boldsymbol{w}^{(t+1)} &= \boldsymbol{w}^{(t)} - \eta \frac{1}{B} \sum_{i \in \mathcal{B}} \nabla_{\boldsymbol{w}} \mathcal{L}^{(i)}(\boldsymbol{w}^{(t)})
\end{align}

where $\mathcal{B}$ is a mini-batch of size $B$.

\section{Regularization}

\subsection{Weight Decay (L2 Regularization)}

\begin{align}
\mathcal{L}_{\text{reg}}(\boldsymbol{w}) &= \mathcal{L}(\boldsymbol{w}) + \frac{\lambda}{2} \|\boldsymbol{w}\|^2
\end{align}

\subsection{Early Stopping}

Monitor validation loss and stop training when it starts increasing:

\begin{align}
\text{Stop if } \mathcal{L}_{\text{val}}^{(t)} > \mathcal{L}_{\text{val}}^{(t-p)} \text{ for } p \text{ consecutive epochs}
\end{align}

\section{Scientific Machine Learning Applications}

\subsection{Constitutive Modeling}

MLPs can learn complex stress-strain relationships:

\begin{align}
\boldsymbol{\sigma} &= \text{MLP}(\boldsymbol{\varepsilon}, \boldsymbol{\theta}) \\
\text{where } \boldsymbol{\theta} &= \{\boldsymbol{W}^{(l)}, \boldsymbol{b}^{(l)}\}_{l=1}^{L}
\end{align}

\subsection{Bingham Plastic Model}

For viscoplastic fluids:

\begin{align}
\tau &= \tau_y + \mu \dot{\gamma} \quad \text{(Traditional)} \\
\tau &= \text{MLP}(\dot{\gamma}, \tau_y, \mu) \quad \text{(Neural Network)}
\end{align}

\subsection{Herschel-Bulkley Model}

For non-Newtonian fluids:

\begin{align}
\tau &= \tau_0 + k \dot{\gamma}^n \quad \text{(Traditional)} \\
\tau &= \text{MLP}(\dot{\gamma}, \tau_0, k, n) \quad \text{(Neural Network)}
\end{align}

\section{Implementation Guidelines}

\subsection{NumPy Implementation}

\begin{itemize}
    \item Use proper weight initialization (Xavier/He initialization)
    \item Implement forward and backward passes
    \item Monitor training and validation losses
    \item Use appropriate learning rates
\end{itemize}

\subsection{PyTorch Implementation}

\begin{itemize}
    \item Define network architecture using \texttt{nn.Module}
    \item Use built-in optimizers (SGD, Adam)
    \item Implement proper data loading and batching
    \item Use GPU acceleration when available
\end{itemize}

\section{Hyperparameter Tuning}

Key hyperparameters to consider:

\begin{itemize}
    \item \textbf{Learning Rate ($\eta$):} Controls step size in gradient descent
    \item \textbf{Batch Size:} Affects gradient estimate quality and training stability
    \item \textbf{Number of Hidden Layers:} Determines network depth
    \item \textbf{Hidden Layer Width:} Number of neurons per layer
    \item \textbf{Activation Functions:} Choice affects gradient flow
    \item \textbf{Regularization Parameters:} Control overfitting
\end{itemize}

\section{Common Challenges and Solutions}

\subsection{Vanishing Gradients}
\begin{itemize}
    \item Use ReLU activations
    \item Proper weight initialization
    \item Batch normalization
    \item Residual connections
\end{itemize}

\subsection{Overfitting}
\begin{itemize}
    \item Regularization (L1/L2)
    \item Dropout
    \item Early stopping
    \item More training data
\end{itemize}

\subsection{Underfitting}
\begin{itemize}
    \item Increase model complexity
    \item Reduce regularization
    \item Train longer
    \item Feature engineering
\end{itemize}

\section{Conclusion}

Multi-Layer Perceptrons are fundamental building blocks in Scientific Machine Learning, providing the ability to:

\begin{itemize}
    \item Approximate complex non-linear functions
    \item Learn from data without explicit programming
    \item Generalize to unseen data
    \item Integrate with physics-based models
\end{itemize}

Understanding MLPs is crucial for developing more advanced neural network architectures and physics-informed neural networks (PINNs) used in scientific computing.

\section{References}

\begin{itemize}
    \item Goodfellow, I., Bengio, Y., \& Courville, A. (2016). Deep Learning. MIT Press.
    \item Haykin, S. (2009). Neural Networks and Learning Machines. Pearson.
    \item Raissi, M., Perdikaris, P., \& Karniadakis, G. E. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378, 686-707.
\end{itemize}

\end{document}