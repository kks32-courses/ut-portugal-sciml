{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Approximation, Nonlinearity, and Activation Functions\n",
    "\n",
    "## Universal Approximation Theorem\n",
    "\n",
    "**Theorem (Cybenko, 1989; Hornik, 1991)**: A single hidden layer neural network with a finite number of neurons can approximate any continuous function on a compact domain to arbitrary accuracy, provided the activation function is non-constant, bounded, and monotonically increasing.\n",
    "\n",
    "Mathematically: For any continuous function $f: [0,1] \\to \\mathbb{R}$ and $\\epsilon > 0$, there exists a network:\n",
    "$$F(x) = \\sum_{i=1}^{N} w_i \\sigma(v_i x + b_i) + w_0$$\n",
    "\n",
    "such that $|F(x) - f(x)| < \\epsilon$ for all $x \\in [0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Nonlinearity is Essential\n",
    "\n",
    "**Key insight**: Without nonlinear activation functions, neural networks collapse to linear models regardless of depth.\n",
    "\n",
    "**Proof**: Consider a network without activation functions:\n",
    "$$h_1 = W_1 x + b_1$$\n",
    "$$h_2 = W_2 h_1 + b_2 = W_2(W_1 x + b_1) + b_2 = (W_2 W_1)x + (W_2 b_1 + b_2)$$\n",
    "\n",
    "This reduces to: $y = W_{eq} x + b_{eq}$ - a linear function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({'font.size': 12, 'figure.figsize': (14, 10)})\n",
    "\n",
    "# Generate complex target function\n",
    "def complex_function(x):\n",
    "    return 0.5 * np.sin(4*np.pi*x) * np.exp(-2*x) + 0.3 * np.cos(6*np.pi*x)\n",
    "\n",
    "x_test = np.linspace(0, 1, 200)\n",
    "y_true = complex_function(x_test)\n",
    "\n",
    "# Training data\n",
    "x_train = np.linspace(0, 1, 20)\n",
    "y_train = complex_function(x_train) + 0.02 * np.random.randn(20)\n",
    "\n",
    "# Convert to tensors\n",
    "x_train_t = torch.tensor(x_train.reshape(-1, 1), dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
    "x_test_t = torch.tensor(x_test.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "print(f\"Target function: Complex oscillatory with exponential decay\")\n",
    "print(f\"Training points: {len(x_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate linear vs nonlinear networks\n",
    "class LinearNetwork(nn.Module):\n",
    "    def __init__(self, width):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, width),\n",
    "            nn.Linear(width, width),\n",
    "            nn.Linear(width, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class NonlinearNetwork(nn.Module):\n",
    "    def __init__(self, width, activation='tanh'):\n",
    "        super().__init__()\n",
    "        if activation == 'tanh':\n",
    "            act = nn.Tanh()\n",
    "        elif activation == 'relu':\n",
    "            act = nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            act = nn.Sigmoid()\n",
    "            \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, width),\n",
    "            act,\n",
    "            nn.Linear(width, width),\n",
    "            act,\n",
    "            nn.Linear(width, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_model(model, x_train, y_train, epochs=3000):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        pred = model(x_train)\n",
    "        loss = criterion(pred, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "# Train models\n",
    "torch.manual_seed(42)\n",
    "linear_net = LinearNetwork(50)\n",
    "tanh_net = NonlinearNetwork(50, 'tanh')\n",
    "relu_net = NonlinearNetwork(50, 'relu')\n",
    "sigmoid_net = NonlinearNetwork(50, 'sigmoid')\n",
    "\n",
    "models = {'Linear': linear_net, 'Tanh': tanh_net, 'ReLU': relu_net, 'Sigmoid': sigmoid_net}\n",
    "\n",
    "print(\"Training models...\")\n",
    "for name, model in models.items():\n",
    "    loss = train_model(model, x_train_t, y_train_t)\n",
    "    print(f\"{name}: Final loss = {loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = [ax1, ax2, ax3, ax4]\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "\n",
    "for ax, (name, model), color in zip(axes, models.items(), colors):\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x_test_t).numpy().flatten()\n",
    "    \n",
    "    ax.plot(x_test, y_true, 'k-', linewidth=3, label='True Function', alpha=0.8)\n",
    "    ax.plot(x_test, y_pred, color=color, linewidth=2, label=f'{name} Network')\n",
    "    ax.scatter(x_train, y_train, color='red', s=30, alpha=0.7, zorder=5)\n",
    "    \n",
    "    mse = np.mean((y_pred - y_true)**2)\n",
    "    ax.text(0.05, 0.95, f'MSE: {mse:.4f}', transform=ax.transAxes,\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    ax.set_title(f'{name} Activation', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions: Properties and Applications\n",
    "\n",
    "### Sigmoid: $\\sigma(x) = \\frac{1}{1+e^{-x}}$\n",
    "- **Range**: (0, 1)\n",
    "- **Use**: Binary classification output\n",
    "- **Issue**: Vanishing gradients\n",
    "\n",
    "### Tanh: $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "- **Range**: (-1, 1)\n",
    "- **Use**: Hidden layers, zero-centered\n",
    "- **Issue**: Vanishing gradients\n",
    "\n",
    "### ReLU: $\\text{ReLU}(x) = \\max(0, x)$\n",
    "- **Range**: [0, âˆž)\n",
    "- **Use**: Most hidden layers (default choice)\n",
    "- **Issue**: Dead neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Universal Approximation with step function approximation\n",
    "def step_function(x):\n",
    "    return (x > 0.3).astype(float) * (x < 0.7).astype(float)\n",
    "\n",
    "# Approximate step function with increasing number of neurons\n",
    "x_fine = np.linspace(0, 1, 500)\n",
    "y_step = step_function(x_fine)\n",
    "\n",
    "# Generate training data for step function\n",
    "x_step_train = np.linspace(0, 1, 30)\n",
    "y_step_train = step_function(x_step_train)\n",
    "x_step_train_t = torch.tensor(x_step_train.reshape(-1, 1), dtype=torch.float32)\n",
    "y_step_train_t = torch.tensor(y_step_train.reshape(-1, 1), dtype=torch.float32)\n",
    "x_fine_t = torch.tensor(x_fine.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# Test different network widths\n",
    "widths = [5, 20, 50, 200]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, width in enumerate(widths):\n",
    "    torch.manual_seed(42)\n",
    "    model = NonlinearNetwork(width, 'tanh')\n",
    "    train_model(model, x_step_train_t, y_step_train_t, epochs=5000)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x_fine_t).numpy().flatten()\n",
    "    \n",
    "    axes[i].plot(x_fine, y_step, 'k-', linewidth=3, label='Target Step Function')\n",
    "    axes[i].plot(x_fine, y_pred, 'b-', linewidth=2, label=f'NN ({width} neurons)')\n",
    "    axes[i].scatter(x_step_train, y_step_train, color='red', s=20, alpha=0.7)\n",
    "    \n",
    "    mse = np.mean((y_pred - y_step)**2)\n",
    "    axes[i].set_title(f'{width} Neurons (MSE: {mse:.4f})', fontweight='bold')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].set_ylim(-0.2, 1.2)\n",
    "\n",
    "plt.suptitle('Universal Approximation: Step Function with Increasing Width', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification example showing ReLU nonlinearity\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Generate moon dataset\n",
    "X, y = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, hidden_size, use_activation=True):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(2, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        self.activation = nn.ReLU() if use_activation else nn.Identity()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# Train linear vs nonlinear classifiers\n",
    "torch.manual_seed(42)\n",
    "linear_clf = Classifier(10, use_activation=False)\n",
    "nonlinear_clf = Classifier(10, use_activation=True)\n",
    "\n",
    "def train_classifier(model, X, y, epochs=2000):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        pred = model(X)\n",
    "        loss = criterion(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "print(\"Training classifiers...\")\n",
    "linear_loss = train_classifier(linear_clf, X_tensor, y_tensor)\n",
    "nonlinear_loss = train_classifier(nonlinear_clf, X_tensor, y_tensor)\n",
    "print(f\"Linear classifier loss: {linear_loss:.4f}\")\n",
    "print(f\"Nonlinear classifier loss: {nonlinear_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classification results\n",
    "def plot_decision_boundary(model, X, y, title):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    grid_points = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        Z = model(grid_points).numpy()\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap='RdYlBu')\n",
    "    plt.colorbar(label='Prediction')\n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "    \n",
    "    colors = ['red', 'blue']\n",
    "    for i in range(2):\n",
    "        plt.scatter(X[y.flatten() == i, 0], X[y.flatten() == i, 1], \n",
    "                   c=colors[i], s=50, alpha=0.8, edgecolors='black')\n",
    "    \n",
    "    plt.title(title, fontweight='bold')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_decision_boundary(linear_clf, X, y, 'Linear Classifier (No Activation)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_decision_boundary(nonlinear_clf, X, y, 'Nonlinear Classifier (ReLU)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracies\n",
    "with torch.no_grad():\n",
    "    linear_pred = (linear_clf(X_tensor) > 0.5).float()\n",
    "    nonlinear_pred = (nonlinear_clf(X_tensor) > 0.5).float()\n",
    "    \n",
    "    linear_acc = (linear_pred == y_tensor).float().mean()\n",
    "    nonlinear_acc = (nonlinear_pred == y_tensor).float().mean()\n",
    "\n",
    "print(f\"\\nClassification Accuracy:\")\n",
    "print(f\"Linear classifier: {linear_acc:.3f}\")\n",
    "print(f\"Nonlinear classifier: {nonlinear_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "### 1. **Universal Approximation is Theoretical**\n",
    "- Single layer networks *can* approximate any function\n",
    "- But may require impractically many neurons\n",
    "- Width requirement grows exponentially with complexity\n",
    "\n",
    "### 2. **Nonlinearity is Fundamental**\n",
    "- Without activation functions: $f(f(f(x))) = f(x)$ (linear)\n",
    "- With activation functions: Infinite representational power\n",
    "- Each neuron creates a different \"basis function\"\n",
    "\n",
    "### 3. **Activation Function Choice Matters**\n",
    "- **Sigmoid/Tanh**: Good for smooth functions, suffer from vanishing gradients\n",
    "- **ReLU**: Efficient, avoids vanishing gradients, risk of dead neurons\n",
    "- **Problem-dependent**: No universal best choice\n",
    "\n",
    "### 4. **Practical Limitations**\n",
    "- Universal approximation doesn't guarantee efficient approximation\n",
    "- Training may not find optimal parameters\n",
    "- Generalization vs. memorization trade-off\n",
    "\n",
    "**Next**: We'll see where single-layer networks fundamentally fail, motivating the need for depth."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
