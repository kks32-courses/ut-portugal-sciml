{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up plot styles\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14, 'figure.figsize': (10, 6)})\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for available device (CUDA, MPS, or CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using Apple Metal Performance Shaders (MPS)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Define the Physics-Informed Neural Network\n",
    "# -----------------------------------------------------------------------------\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=32, output_size=1, num_layers=3):\n",
    "        super(PINN, self).__init__()\n",
    "        \n",
    "        # Build the network layers - smaller network for better training\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_size))\n",
    "        layers.append(nn.Tanh())\n",
    "        \n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.Tanh())\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_size, output_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights with smaller values\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier initialization with small scaling\"\"\"\n",
    "        for m in self.network.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=0.5)  # Smaller initial weights\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Define Loss Functions\n",
    "# -----------------------------------------------------------------------------\n",
    "def pde_loss(model, x_interior):\n",
    "    \"\"\"\n",
    "    Compute the PDE residual loss for: dÂ²u/dxÂ² + Ï€sin(Ï€x) = 0\n",
    "    \"\"\"\n",
    "    x_interior.requires_grad_(True)\n",
    "    \n",
    "    # Forward pass\n",
    "    u = model(x_interior)\n",
    "    \n",
    "    # Compute first derivative\n",
    "    du_dx = torch.autograd.grad(\n",
    "        outputs=u,\n",
    "        inputs=x_interior,\n",
    "        grad_outputs=torch.ones_like(u),\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "    \n",
    "    # Compute second derivative\n",
    "    d2u_dx2 = torch.autograd.grad(\n",
    "        outputs=du_dx,\n",
    "        inputs=x_interior,\n",
    "        grad_outputs=torch.ones_like(du_dx),\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "    \n",
    "    # PDE residual: dÂ²u/dxÂ² + Ï€ sin(Ï€x) = 0\n",
    "    forcing_term = torch.pi * torch.sin(torch.pi * x_interior)\n",
    "    pde_residual = d2u_dx2 + forcing_term\n",
    "    \n",
    "    return torch.mean(pde_residual**2)\n",
    "\n",
    "def boundary_loss(model, x_boundary):\n",
    "    \"\"\"\n",
    "    Compute boundary condition loss: u(0) = 0, u(1) = 0\n",
    "    \"\"\"\n",
    "    u_boundary = model(x_boundary)\n",
    "    return torch.mean(u_boundary**2)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. Training Function\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_pinn(model, epochs=20000, lr=0.001):\n",
    "    \"\"\"\n",
    "    Train the PINN using physics loss and boundary conditions\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=1000)\n",
    "    \n",
    "    # Generate collocation points for PDE residual (interior points)\n",
    "    n_interior = 500\n",
    "    n_boundary = 10  # More boundary points for better BC enforcement\n",
    "    \n",
    "    print(f\"\\n--- Training PINN ---\")\n",
    "    print(f\"Interior collocation points: {n_interior}\")\n",
    "    print(f\"Boundary enforcement points: {n_boundary}\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    losses = {'total': [], 'pde': [], 'boundary': []}\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(range(epochs), desc=\"Training PINN\", ncols=100)\n",
    "    \n",
    "    for epoch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Regenerate collocation points every few epochs for better coverage\n",
    "        if epoch % 100 == 0:\n",
    "            x_interior = torch.rand(n_interior, 1, device=device) * 0.98 + 0.01\n",
    "            x_boundary = torch.cat([\n",
    "                torch.zeros(n_boundary//2, 1, device=device),\n",
    "                torch.ones(n_boundary//2, 1, device=device)\n",
    "            ])\n",
    "        \n",
    "        # Compute PDE residual loss\n",
    "        loss_pde = pde_loss(model, x_interior)\n",
    "        \n",
    "        # Compute boundary condition loss\n",
    "        loss_bc = boundary_loss(model, x_boundary)\n",
    "        \n",
    "        # Total loss with adaptive weighting\n",
    "        lambda_bc = 1000.0  # Higher weight for boundary conditions\n",
    "        total_loss = loss_pde + lambda_bc * loss_bc\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent explosion\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step(total_loss)\n",
    "        \n",
    "        # Store losses\n",
    "        losses['total'].append(total_loss.item())\n",
    "        losses['pde'].append(loss_pde.item())\n",
    "        losses['boundary'].append(loss_bc.item())\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{total_loss.item():.2e}',\n",
    "            'PDE': f'{loss_pde.item():.2e}',\n",
    "            'BC': f'{loss_bc.item():.2e}'\n",
    "        })\n",
    "    \n",
    "    pbar.close()\n",
    "    return model, losses\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. Analytical Solution for Comparison\n",
    "# -----------------------------------------------------------------------------\n",
    "def analytical_solution(x):\n",
    "    \"\"\"Analytical solution: u(x) = (1/Ï€)sin(Ï€x)\"\"\"\n",
    "    return (1 / (np.pi)) * np.sin(np.pi * x)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. Main Training and Visualization\n",
    "# -----------------------------------------------------------------------------\n",
    "def main():\n",
    "    # Create the PINN model - smaller network for better convergence\n",
    "    model = PINN(input_size=1, hidden_size=32, output_size=1, num_layers=3).to(device)\n",
    "    \n",
    "    print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    \n",
    "    # Train the model with lower learning rate\n",
    "    trained_model, losses = train_pinn(model, epochs=15000, lr=0.0005)\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ Training completed!\")\n",
    "    \n",
    "    # Generate test points for visualization\n",
    "    x_test = torch.linspace(0, 1, 200, device=device).reshape(-1, 1)\n",
    "    \n",
    "    # Get predictions\n",
    "    trained_model.eval()\n",
    "    with torch.no_grad():\n",
    "        u_pred = trained_model(x_test).cpu().numpy()\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    x_test_np = x_test.cpu().numpy()\n",
    "    u_true = analytical_solution(x_test_np)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: Solution comparison\n",
    "    ax1.plot(x_test_np, u_true, 'k-', label='Analytical Solution', linewidth=3)\n",
    "    ax1.plot(x_test_np, u_pred, 'r--', label='PINN Prediction', linewidth=2.5)\n",
    "    ax1.set_xlabel('Position (x)')\n",
    "    ax1.set_ylabel('Temperature (u)')\n",
    "    ax1.set_title('PINN vs Analytical Solution')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Training losses (smoothed for better visualization)\n",
    "    epochs = range(1, len(losses['total']) + 1)\n",
    "    # Apply smoothing to reduce noise in loss plots\n",
    "    window = 100\n",
    "    if len(losses['total']) > window:\n",
    "        total_smooth = np.convolve(losses['total'], np.ones(window)/window, mode='valid')\n",
    "        pde_smooth = np.convolve(losses['pde'], np.ones(window)/window, mode='valid')\n",
    "        boundary_smooth = np.convolve(losses['boundary'], np.ones(window)/window, mode='valid')\n",
    "        epochs_smooth = range(window, len(losses['total']) + 1)\n",
    "        \n",
    "        ax2.semilogy(epochs_smooth, total_smooth, 'b-', label='Total Loss (smoothed)', linewidth=2)\n",
    "        ax2.semilogy(epochs_smooth, pde_smooth, 'g--', label='PDE Loss (smoothed)', linewidth=2)\n",
    "        ax2.semilogy(epochs_smooth, boundary_smooth, 'r:', label='Boundary Loss (smoothed)', linewidth=2)\n",
    "    else:\n",
    "        ax2.semilogy(epochs, losses['total'], 'b-', label='Total Loss', linewidth=2)\n",
    "        ax2.semilogy(epochs, losses['pde'], 'g--', label='PDE Loss', linewidth=2)\n",
    "        ax2.semilogy(epochs, losses['boundary'], 'r:', label='Boundary Loss', linewidth=2)\n",
    "    \n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('Training Loss Evolution')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute and print error metrics\n",
    "    error = np.abs(u_pred.flatten() - u_true.flatten())\n",
    "    max_error = np.max(error)\n",
    "    mean_error = np.mean(error)\n",
    "    rmse = np.sqrt(np.mean(error**2))\n",
    "    \n",
    "    print(\"\\n=== PINN Performance Metrics ===\")\n",
    "    print(f\"Maximum absolute error: {max_error:.2e}\")\n",
    "    print(f\"Mean absolute error: {mean_error:.2e}\")\n",
    "    print(f\"Root Mean Square Error: {rmse:.2e}\")\n",
    "    print(f\"Final total loss: {losses['total'][-1]:.2e}\")\n",
    "    print(f\"Final PDE loss: {losses['pde'][-1]:.2e}\")\n",
    "    print(f\"Final boundary loss: {losses['boundary'][-1]:.2e}\")\n",
    "    \n",
    "    # Test boundary conditions\n",
    "    with torch.no_grad():\n",
    "        u_0 = trained_model(torch.tensor([[0.0]], device=device)).item()\n",
    "        u_1 = trained_model(torch.tensor([[1.0]], device=device)).item()\n",
    "    \n",
    "    print(f\"\\n=== Boundary Condition Check ===\")\n",
    "    print(f\"u(0) = {u_0:.2e} (should be 0)\")\n",
    "    print(f\"u(1) = {u_1:.2e} (should be 0)\")\n",
    "    \n",
    "    # Device info summary\n",
    "    print(f\"\\n=== Device Information ===\")\n",
    "    print(f\"Device used: {device}\")\n",
    "    if device.type == 'cuda':\n",
    "        print(f\"GPU Memory allocated: {torch.cuda.memory_allocated(device)/1024**2:.1f} MB\")\n",
    "        print(f\"GPU Memory cached: {torch.cuda.memory_reserved(device)/1024**2:.1f} MB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5fe56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
