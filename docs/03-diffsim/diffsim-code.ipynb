{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook (`mlp.ipynb`), we introduced Automatic Differentiation (AD) as the core engine that enables the training of neural networks by computing gradients of a loss function with respect to network parameters. \n",
    "\n",
    "However, the power of AD extends far beyond just neural networks. It allows us to make **entire physical simulations differentiable**. This paradigm, often called **Differentiable Simulation** or **Differentiable Physics**, involves implementing a simulator (e.g., a PDE solver) in a framework that supports AD, such as PyTorch or JAX. By doing so, we can automatically compute the gradient of a final quantity (like a measurement or a loss function) with respect to any initial parameter of the simulation.\n",
    "\n",
    "This notebook demonstrates this powerful concept. We will:\n",
    "1. Briefly recall how gradients are computed in PyTorch.\n",
    "2. Introduce the JAX framework for high-performance differentiable programming.\n",
    "3. Build a differentiable simulator for the 1D acoustic wave equation.\n",
    "4. Use this simulator to solve a challenging inverse problem: Full Waveform Inversion (FWI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Reminder: Gradients in PyTorch\n",
    "\n",
    "As we saw previously, frameworks like PyTorch keep track of all operations on tensors. When we call `.backward()` on a final scalar output (like a loss), PyTorch uses reverse-mode AD (backpropagation) to compute the gradient of that output with respect to the inputs that have `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx1: 4.0\n",
      "dy/dx2: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define variables that require gradients\n",
    "x1 = torch.tensor(2.0, requires_grad=True)\n",
    "x2 = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Define a simple function\n",
    "y = x1**2 + x2\n",
    "\n",
    "# Compute gradients using reverse mode AD\n",
    "y.backward()\n",
    "\n",
    "# Access the computed gradients\n",
    "print(f\"dy/dx1: {x1.grad.item()}\")\n",
    "print(f\"dy/dx2: {x2.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This same principle applies not just to a single function, but to a whole sequence of operations, such as the time-stepping loop in a physics simulator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAX: High-Performance Differentiable Programming\n",
    "\n",
    "For our differentiable simulation, we will use **JAX**, a library from Google for high-performance numerical computing. JAX combines a NumPy-like API with a powerful set of transformations:\n",
    "\n",
    "- **`grad()`**: Automatic differentiation, just like in PyTorch.\n",
    "- **`jit()`**: Just-in-time (JIT) compilation to accelerate Python code (especially loops) on CPUs, GPUs, and TPUs.\n",
    "- **`vmap()`**: Automatic vectorization of functions.\n",
    "\n",
    "This combination makes JAX exceptionally well-suited for writing fast and differentiable physics simulators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "XlaRuntimeError",
     "evalue": "UNIMPLEMENTED: default_memory_space is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXlaRuntimeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# grad() transformation\u001b[39;00m\n\u001b[32m     10\u001b[39m df = grad(f)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGradient of sin(x) at x=1.0 is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# jit() transformation\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;129m@jit\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfast_f\u001b[39m(x):\n",
      "    \u001b[31m[... skipping hidden 18 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mf\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mf\u001b[39m(x):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 26 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/CE397-Scientific-MachineLearning/utp-sciml/env/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:244\u001b[39m, in \u001b[36mbatched_device_put\u001b[39m\u001b[34m(aval, sharding, xs, devices, committed)\u001b[39m\n\u001b[32m    241\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bufs) == \u001b[38;5;28mlen\u001b[39m(xs) > \u001b[32m0\u001b[39m:\n\u001b[32m    242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m array.ArrayImpl(\n\u001b[32m    243\u001b[39m         aval, sharding, bufs, committed=committed, _skip_checks=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatched_device_put\u001b[49m\u001b[43m(\u001b[49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommitted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    246\u001b[39m   util.test_event(\u001b[33m\"\u001b[39m\u001b[33mbatched_device_put_end\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mXlaRuntimeError\u001b[39m: UNIMPLEMENTED: default_memory_space is not supported."
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return jnp.sin(x)\n",
    "\n",
    "# grad() transformation\n",
    "df = grad(f)\n",
    "print(f\"Gradient of sin(x) at x=1.0 is: {df(1.0)}\")\n",
    "\n",
    "# jit() transformation\n",
    "@jit\n",
    "def fast_f(x):\n",
    "    return jnp.sin(x)\n",
    "\n",
    "print(f\"JIT-compiled sin(x) at x=1.0 is: {fast_f(1.0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Differentiable 1D Wave Simulation\n",
    "\n",
    "We will now build a differentiable simulator for the 1D acoustic wave equation. This equation describes how waves (like sound or seismic waves) propagate through a medium.\n",
    "\n",
    "$$ \\frac{\\partial^2 u}{\\partial t^2} = c^2 \\frac{\\partial^2 u}{\\partial x^2} $$\n",
    "\n",
    "Where:\n",
    "- $ u(x, t) $ is the wave's displacement or pressure at position $x$ and time $t$.\n",
    "- $ c $ is the wave speed in the medium, which can vary with position, $c(x)$.\n",
    "\n",
    "### The Forward Problem: Simulation\n",
    "The forward problem is to simulate the behavior of $u(x,t)$ given an initial state and the wave speed profile $c(x)$. We will solve this using a finite difference method. By rearranging the central difference approximation, we can find the wave's state at the next timestep based on its two previous states:\n",
    "\n",
    "$$u_i^{n+1} = c_i^2 \\frac{\\Delta t^2}{\\Delta x^2} (u_{i+1}^n - 2u_i^n + u_{i-1}^n) + 2u_i^n - u_i^{n-1} $$\n",
    "\n",
    "We can implement this time-stepping loop in JAX. Using `@jit`, this loop will be compiled for high performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "XlaRuntimeError",
     "evalue": "UNIMPLEMENTED: default_memory_space is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXlaRuntimeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m n = \u001b[32m1000\u001b[39m\n\u001b[32m      3\u001b[39m dx = \u001b[32m1.0\u001b[39m/(n-\u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m x0 = \u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;129m@jit\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwave_propagation\u001b[39m(params):\n\u001b[32m      8\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Simulates 1D wave propagation using a finite difference scheme.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/CE397-Scientific-MachineLearning/utp-sciml/env/lib/python3.11/site-packages/jax/_src/numpy/array_creation.py:503\u001b[39m, in \u001b[36mlinspace\u001b[39m\u001b[34m(start, stop, num, endpoint, retstep, dtype, axis, device)\u001b[39m\n\u001b[32m    501\u001b[39m num = core.concrete_dim_or_error(num, \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mnum\u001b[39m\u001b[33m'\u001b[39m\u001b[33m argument of jnp.linspace\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    502\u001b[39m axis = core.concrete_or_error(operator.index, axis, \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33maxis\u001b[39m\u001b[33m'\u001b[39m\u001b[33m argument of jnp.linspace\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_linspace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 10 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/CE397-Scientific-MachineLearning/utp-sciml/env/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:244\u001b[39m, in \u001b[36mbatched_device_put\u001b[39m\u001b[34m(aval, sharding, xs, devices, committed)\u001b[39m\n\u001b[32m    241\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bufs) == \u001b[38;5;28mlen\u001b[39m(xs) > \u001b[32m0\u001b[39m:\n\u001b[32m    242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m array.ArrayImpl(\n\u001b[32m    243\u001b[39m         aval, sharding, bufs, committed=committed, _skip_checks=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatched_device_put\u001b[49m\u001b[43m(\u001b[49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommitted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    246\u001b[39m   util.test_event(\u001b[33m\"\u001b[39m\u001b[33mbatched_device_put_end\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mXlaRuntimeError\u001b[39m: UNIMPLEMENTED: default_memory_space is not supported."
     ]
    }
   ],
   "source": [
    "# Set up an n-point uniform mesh\n",
    "n = 1000\n",
    "dx = 1.0/(n-1)\n",
    "x0 = jnp.linspace(0.0, 1.0, n)\n",
    "\n",
    "@jit\n",
    "def wave_propagation(params):\n",
    "    \"\"\"Simulates 1D wave propagation using a finite difference scheme.\"\"\"\n",
    "    c = params # c can be a scalar or a vector for a spatially varying profile\n",
    "    dt = 5e-4\n",
    "    C = c * dt / dx\n",
    "    C2 = C**2\n",
    "\n",
    "    # Set up initial conditions (a Gaussian pulse)\n",
    "    u0 = jnp.exp(-(5 * (x0 - 0.5))**2)\n",
    "    u1 = jnp.exp(-(5 * (x0 - 0.5 - c * dt))**2)\n",
    "    u2 = jnp.zeros(n)\n",
    "\n",
    "    def step(i, carry):\n",
    "        u0, u1, _ = carry\n",
    "        # Get neighbors using jnp.roll for periodic boundaries, then fix for Dirichlet\n",
    "        u1p = jnp.roll(u1, 1)\n",
    "        u1p = u1p.at[0].set(0)\n",
    "        u1n = jnp.roll(u1, -1)\n",
    "        u1n = u1n.at[n - 1].set(0)\n",
    "        \n",
    "        # Central difference update rule\n",
    "        u2 = 2 * u1 - u0 + C2 * (u1p - 2 * u1 + u1n)\n",
    "        u0, u1 = u1, u2\n",
    "        return (u0, u1, u2)\n",
    "\n",
    "    # Run the simulation loop\n",
    "    u0, u1, u2 = lax.fori_loop(0, 5000, step, (u0, u1, u2))\n",
    "    return u2\n",
    "\n",
    "# --- Run a forward simulation ---\n",
    "ctarget = 1.0 # Constant velocity profile\n",
    "target_wave = wave_propagation(ctarget)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x0, target_wave, 'b-', label='Final Wave State (u2)')\n",
    "plt.title(f'Forward Simulation with c = {ctarget}')\n",
    "plt.xlabel('Position x')\n",
    "plt.ylabel('Displacement u')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Inverse Problem: Full Waveform Inversion (FWI)\n",
    "\n",
    "Now for the exciting part. The inverse problem asks: **If we measure the final wave `target_wave`, can we determine the velocity profile `c` that produced it?**\n",
    "\n",
    "This is a classic and difficult problem in geophysics. With a differentiable simulator, we can solve it using gradient descent.\n",
    "\n",
    "1.  **Define a Loss Function**: We need a way to measure the difference between our simulation's output and the observed data. The L2 norm (mean squared error) is a standard choice.\n",
    "    $$L(c) = || \\text{wave_propagation}(c) - \\text{target_wave} ||^2$$\n",
    "2.  **Compute the Gradient**: Because our `wave_propagation` function is written in JAX, we can get the gradient of the loss with respect to the parameters `c` for free: `grad(L)(c)`.\n",
    "3.  **Optimize**: We start with an initial guess for `c` and iteratively update it by moving in the direction of the negative gradient, using an optimizer like Adam.\n",
    "\n",
    "Let's try to recover the constant velocity profile `ctarget = 1.0` starting from a wrong guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "@jit\n",
    "def compute_loss(c):\n",
    "    u2_simulated = wave_propagation(c)\n",
    "    return jnp.linalg.norm(u2_simulated - target_wave)\n",
    "\n",
    "# Define the gradient of the loss function\n",
    "loss_grad_fn = jit(grad(compute_loss))\n",
    "\n",
    "# --- Setup the optimization ---\n",
    "learning_rate = 1e-3\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# Initial guess for the velocity (wrong value)\n",
    "params_c = 0.85 \n",
    "opt_state = optimizer.init(params_c)\n",
    "\n",
    "print(f\"Target velocity c: {ctarget}\")\n",
    "print(f\"Initial guess for c: {params_c}\")\n",
    "\n",
    "# --- Run the optimization loop ---\n",
    "for i in range(1001):\n",
    "    grads = loss_grad_fn(params_c)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params_c = optax.apply_updates(params_c, updates)\n",
    "    if i % 200 == 0:\n",
    "        loss = compute_loss(params_c)\n",
    "        print(f\"Iteration {i}, Loss: {loss:.6f}, Current c: {params_c:.6f}\")\n",
    "\n",
    "print(f\"\\nFinal recovered velocity c: {params_c:.6f}\")\n",
    "\n",
    "# --- Visualize the results ---\n",
    "recovered_wave = wave_propagation(params_c)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x0, target_wave, 'k-', label='Target Wave', linewidth=2)\n",
    "plt.plot(x0, recovered_wave, 'r--', label='Recovered Wave', linewidth=2)\n",
    "plt.title('FWI Result: Target vs. Recovered Wave')\n",
    "plt.xlabel('Position x')\n",
    "plt.ylabel('Displacement u')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Case: Recovering a Spatially-Varying Profile\n",
    "\n",
    "The true power of this method becomes apparent when we try to recover a more complex, spatially-varying velocity profile. Let's define a target velocity `c(x)` that changes linearly across the domain.\n",
    "\n",
    "Amazingly, **no change to our `compute_loss` or gradient descent logic is needed**. The `wave_propagation` function and the AD framework handle the fact that `c` is now a vector of parameters instead of a single scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New target: a linear velocity profile\n",
    "ctarget_linear = jnp.linspace(0.9, 1.1, n)\n",
    "target_wave_linear = wave_propagation(ctarget_linear)\n",
    "\n",
    "# The loss function remains the same, but now computes the loss against the new target\n",
    "@jit\n",
    "def compute_loss_linear(c_vector):\n",
    "    u2_simulated = wave_propagation(c_vector)\n",
    "    return jnp.linalg.norm(u2_simulated - target_wave_linear)\n",
    "\n",
    "loss_grad_fn_linear = jit(grad(compute_loss_linear))\n",
    "\n",
    "# --- Setup and run optimization for the vector case ---\n",
    "learning_rate = 1e-3\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# Initial guess: a constant, incorrect velocity profile\n",
    "params_c_vector = jnp.ones(n) * 0.85\n",
    "opt_state = optimizer.init(params_c_vector)\n",
    "\n",
    "print(\"Starting optimization to find the linear velocity profile...\")\n",
    "for i in range(2001):\n",
    "    grads = loss_grad_fn_linear(params_c_vector)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params_c_vector = optax.apply_updates(params_c_vector, updates)\n",
    "    if i % 500 == 0:\n",
    "        loss = compute_loss_linear(params_c_vector)\n",
    "        print(f\"Iteration {i}, Loss: {loss:.4f}\")\n",
    "\n",
    "print(\"\\nOptimization finished!\")\n",
    "\n",
    "# --- Visualize the results for the linear profile ---\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot the recovered velocity profile\n",
    "ax1.plot(x0, ctarget_linear, 'k-', label='Target Profile', linewidth=2)\n",
    "ax1.plot(x0, params_c_vector, 'r--', label='Recovered Profile', linewidth=2)\n",
    "ax1.set_title('Velocity Profile Recovery (c(x))')\n",
    "ax1.set_xlabel('Position x')\n",
    "ax1.set_ylabel('Wave Speed c')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.5)\n",
    "\n",
    "# Plot the resulting waves\n",
    "recovered_wave_linear = wave_propagation(params_c_vector)\n",
    "ax2.plot(x0, target_wave_linear, 'k-', label='Target Wave', linewidth=2)\n",
    "ax2.plot(x0, recovered_wave_linear, 'r--', label='Recovered Wave', linewidth=2)\n",
    "ax2.set_title('Final Wave Comparison')\n",
    "ax2.set_xlabel('Position x')\n",
    "ax2.set_ylabel('Displacement u')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the core idea of differentiable simulation.\n",
    "\n",
    "By implementing a physics simulator within a framework that supports automatic differentiation (like JAX), we can efficiently solve complex, gradient-based inverse problems. We simply define a loss function that measures the difference between our simulation's output and some observed data, and then use an optimizer to minimize this loss by adjusting the physical parameters of the simulation.\n",
    "\n",
    "This powerful technique is a cornerstone of modern Scientific Machine Learning (SciML), enabling the fusion of traditional scientific models with data-driven methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
